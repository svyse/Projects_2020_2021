{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "global-dayton",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indoor-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# for building DQN model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-output",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medieval-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "productive-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extended-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.05\n",
      "7.94\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(round(Time_matrix.mean(), 2))\n",
    "print(round(Time_matrix.var(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-tumor",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-manitoba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-electron",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-bookmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helpful-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-garlic",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "printable-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.9\n",
    "        self.learning_rate = 0.01 \n",
    "        \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon= 1\n",
    "        self.epsilon_decay = 4e-3\n",
    "        self.epsilon_min = 0.0005\n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state, env):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment\n",
    "        possible_actions_index, actions= env.requests(state)\n",
    "        possible_actions_index.append(0)\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            \n",
    "            return random.sample(possible_actions_index,1)[0]\n",
    "        else:\n",
    "            state= state.reshape(1, self.state_size)\n",
    "            \n",
    "            q_val= self.model.predict(state)\n",
    "            \n",
    "            return np.where(q_val[0] == np.max(np.array([q_val[0][i] for i in possible_actions_index])))[0][0]\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        # Write your code here:\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self, env):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            state_out = np.zeros((self.batch_size, self.state_size))# write here\n",
    "            state_in = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_bool = mini_batch[i]\n",
    "                #state_encod = env.state_encod_arch2(state,action)\n",
    "                \n",
    "                state_in[i] = state\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_bool)\n",
    "                state_out[i] = next_state\n",
    "                           \n",
    "                \n",
    "                # Write your code from here\n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(state_in)\n",
    "                \n",
    "                \n",
    "            # 2. Get the target for the Q-network\n",
    "            target_qval = self.model.predict(state_out)\n",
    "                \n",
    "                \n",
    "            #3. Update your 'state_out' and 'state_in' batch. Be careful to use the encoded state-action pair\n",
    "                \n",
    "            for i in range(self.batch_size):\n",
    "                next_possible_actions_index,_ = env.requests(state_out[i])\n",
    "                next_possible_actions_index.append(0)\n",
    "                \n",
    "                if not done[i]:\n",
    "                        target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(np.array([target_qval[i][j] for j in next_possible_actions_index]))\n",
    "                else:\n",
    "                        target[i][actions[i]] = rewards[i]\n",
    "                \n",
    "                \n",
    "            # 4. Fit your model and track the loss values\n",
    "            \n",
    "            self.model.fit(state_in, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "                \n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "animated-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-circular",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expressed-algebra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State is  [3, 22, 1]\n",
      "episode 0, reward -375.0, memory_length 130, epsilon 1.0, time 731.0, rides 129\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 1, reward -434.0, memory_length 257, epsilon 0.996, time 725.0, rides 126\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 2, reward -176.0, memory_length 385, epsilon 0.992016, time 736.0, rides 127\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 3, reward 4.0, memory_length 521, epsilon 0.988047936, time 731.0, rides 135\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 4, reward -399.0, memory_length 644, epsilon 0.984095744256, time 730.0, rides 122\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 5, reward -367.0, memory_length 767, epsilon 0.980159361278976, time 728.0, rides 122\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 6, reward -349.0, memory_length 906, epsilon 0.9762387238338601, time 721.0, rides 138\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 7, reward -353.0, memory_length 1033, epsilon 0.9723337689385246, time 738.0, rides 126\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 8, reward -292.0, memory_length 1155, epsilon 0.9684444338627706, time 725.0, rides 121\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 9, reward -185.0, memory_length 1278, epsilon 0.9645706561273194, time 730.0, rides 122\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 10, reward -365.0, memory_length 1419, epsilon 0.9607123735028101, time 725.0, rides 140\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 11, reward -137.0, memory_length 1535, epsilon 0.9568695240087989, time 723.0, rides 115\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 12, reward -104.0, memory_length 1652, epsilon 0.9530420459127638, time 729.0, rides 116\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 13, reward -239.0, memory_length 1780, epsilon 0.9492298777291127, time 738.0, rides 127\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 14, reward 56.0, memory_length 1903, epsilon 0.9454329582181962, time 730.0, rides 122\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 15, reward 11.0, memory_length 2000, epsilon 0.9416512263853234, time 730.0, rides 140\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 16, reward -444.0, memory_length 2000, epsilon 0.9378846214797821, time 731.0, rides 133\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 17, reward -428.0, memory_length 2000, epsilon 0.934133082993863, time 722.0, rides 114\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 18, reward 28.0, memory_length 2000, epsilon 0.9303965506618875, time 729.0, rides 129\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 19, reward -286.0, memory_length 2000, epsilon 0.92667496445924, time 739.0, rides 135\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 20, reward -224.0, memory_length 2000, epsilon 0.922968264601403, time 733.0, rides 135\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 21, reward -469.0, memory_length 2000, epsilon 0.9192763915429975, time 728.0, rides 123\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 22, reward -146.0, memory_length 2000, epsilon 0.9155992859768254, time 725.0, rides 117\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 23, reward -174.0, memory_length 2000, epsilon 0.9119368888329181, time 725.0, rides 145\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 24, reward -159.0, memory_length 2000, epsilon 0.9082891412775864, time 729.0, rides 133\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 25, reward -264.0, memory_length 2000, epsilon 0.9046559847124761, time 731.0, rides 132\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 26, reward -194.0, memory_length 2000, epsilon 0.9010373607736262, time 735.0, rides 126\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 27, reward -275.0, memory_length 2000, epsilon 0.8974332113305317, time 731.0, rides 134\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 28, reward -86.0, memory_length 2000, epsilon 0.8938434784852095, time 734.0, rides 131\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 29, reward -174.0, memory_length 2000, epsilon 0.8902681045712687, time 725.0, rides 144\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 30, reward -416.0, memory_length 2000, epsilon 0.8867070321529836, time 725.0, rides 129\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 31, reward -363.0, memory_length 2000, epsilon 0.8831602040243717, time 725.0, rides 125\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 32, reward -167.0, memory_length 2000, epsilon 0.8796275632082742, time 731.0, rides 118\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 33, reward 92.0, memory_length 2000, epsilon 0.8761090529554411, time 732.0, rides 116\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 34, reward -587.0, memory_length 2000, epsilon 0.8726046167436193, time 737.0, rides 121\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 35, reward -383.0, memory_length 2000, epsilon 0.8691141982766448, time 727.0, rides 127\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 36, reward -144.0, memory_length 2000, epsilon 0.8656377414835382, time 722.0, rides 133\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 37, reward -374.0, memory_length 2000, epsilon 0.862175190517604, time 731.0, rides 147\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 38, reward -319.0, memory_length 2000, epsilon 0.8587264897555337, time 730.0, rides 122\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 39, reward -189.0, memory_length 2000, epsilon 0.8552915837965116, time 722.0, rides 119\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 40, reward -89.0, memory_length 2000, epsilon 0.8518704174613255, time 727.0, rides 126\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 41, reward -223.0, memory_length 2000, epsilon 0.8484629357914802, time 732.0, rides 141\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 42, reward 250.0, memory_length 2000, epsilon 0.8450690840483143, time 727.0, rides 121\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 43, reward -18.0, memory_length 2000, epsilon 0.841688807712121, time 724.0, rides 130\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 44, reward -234.0, memory_length 2000, epsilon 0.8383220524812726, time 728.0, rides 134\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 45, reward 115.0, memory_length 2000, epsilon 0.8349687642713475, time 723.0, rides 122\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 46, reward -62.0, memory_length 2000, epsilon 0.831628889214262, time 731.0, rides 135\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 47, reward -211.0, memory_length 2000, epsilon 0.828302373657405, time 732.0, rides 131\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 48, reward -474.0, memory_length 2000, epsilon 0.8249891641627753, time 735.0, rides 155\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 49, reward -267.0, memory_length 2000, epsilon 0.8216892075061243, time 727.0, rides 127\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 50, reward -191.0, memory_length 2000, epsilon 0.8184024506760997, time 726.0, rides 113\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 51, reward -224.0, memory_length 2000, epsilon 0.8151288408733953, time 732.0, rides 134\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 52, reward -56.0, memory_length 2000, epsilon 0.8118683255099017, time 722.0, rides 141\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 53, reward -258.0, memory_length 2000, epsilon 0.8086208522078622, time 721.0, rides 141\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 54, reward -188.0, memory_length 2000, epsilon 0.8053863687990307, time 722.0, rides 130\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 55, reward -216.0, memory_length 2000, epsilon 0.8021648233238345, time 744.0, rides 117\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 56, reward -197.0, memory_length 2000, epsilon 0.7989561640305393, time 729.0, rides 119\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 57, reward -239.0, memory_length 2000, epsilon 0.795760339374417, time 731.0, rides 128\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 58, reward -159.0, memory_length 2000, epsilon 0.7925772980169195, time 728.0, rides 120\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 59, reward -138.0, memory_length 2000, epsilon 0.7894069888248517, time 734.0, rides 128\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 60, reward -385.0, memory_length 2000, epsilon 0.7862493608695523, time 732.0, rides 117\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 61, reward -146.0, memory_length 2000, epsilon 0.7831043634260741, time 737.0, rides 129\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 62, reward -330.0, memory_length 2000, epsilon 0.7799719459723699, time 725.0, rides 128\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 63, reward 138.0, memory_length 2000, epsilon 0.7768520581884804, time 725.0, rides 128\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 64, reward -155.0, memory_length 2000, epsilon 0.7737446499557264, time 736.0, rides 135\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 65, reward -17.0, memory_length 2000, epsilon 0.7706496713559035, time 730.0, rides 119\n",
      "Initial State is  [0, 8, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 66, reward -167.0, memory_length 2000, epsilon 0.7675670726704799, time 727.0, rides 128\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 67, reward -31.0, memory_length 2000, epsilon 0.764496804379798, time 735.0, rides 125\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 68, reward -69.0, memory_length 2000, epsilon 0.7614388171622788, time 724.0, rides 140\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 69, reward -24.0, memory_length 2000, epsilon 0.7583930618936296, time 731.0, rides 113\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 70, reward -207.0, memory_length 2000, epsilon 0.7553594896460551, time 729.0, rides 148\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 71, reward -260.0, memory_length 2000, epsilon 0.7523380516874709, time 729.0, rides 119\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 72, reward -187.0, memory_length 2000, epsilon 0.749328699480721, time 734.0, rides 119\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 73, reward 106.0, memory_length 2000, epsilon 0.7463313846827981, time 729.0, rides 120\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 74, reward -53.0, memory_length 2000, epsilon 0.7433460591440669, time 725.0, rides 110\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 75, reward -25.0, memory_length 2000, epsilon 0.7403726749074907, time 726.0, rides 143\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 76, reward 29.0, memory_length 2000, epsilon 0.7374111842078607, time 728.0, rides 140\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 77, reward -277.0, memory_length 2000, epsilon 0.7344615394710292, time 742.0, rides 131\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 78, reward 46.0, memory_length 2000, epsilon 0.7315236933131452, time 723.0, rides 131\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 79, reward 3.0, memory_length 2000, epsilon 0.7285975985398926, time 726.0, rides 107\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 80, reward -183.0, memory_length 2000, epsilon 0.725683208145733, time 730.0, rides 119\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 81, reward -173.0, memory_length 2000, epsilon 0.72278047531315, time 734.0, rides 129\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 82, reward -161.0, memory_length 2000, epsilon 0.7198893534118974, time 728.0, rides 134\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 83, reward -128.0, memory_length 2000, epsilon 0.7170097959982499, time 729.0, rides 114\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 84, reward -204.0, memory_length 2000, epsilon 0.7141417568142568, time 730.0, rides 133\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 85, reward -104.0, memory_length 2000, epsilon 0.7112851897869998, time 727.0, rides 122\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 86, reward 4.0, memory_length 2000, epsilon 0.7084400490278518, time 730.0, rides 121\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 87, reward 52.0, memory_length 2000, epsilon 0.7056062888317404, time 734.0, rides 120\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 88, reward 117.0, memory_length 2000, epsilon 0.7027838636764134, time 729.0, rides 132\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 89, reward 65.0, memory_length 2000, epsilon 0.6999727282217078, time 732.0, rides 126\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 90, reward 29.0, memory_length 2000, epsilon 0.697172837308821, time 735.0, rides 135\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 91, reward -25.0, memory_length 2000, epsilon 0.6943841459595856, time 723.0, rides 119\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 92, reward -11.0, memory_length 2000, epsilon 0.6916066093757474, time 724.0, rides 138\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 93, reward 2.0, memory_length 2000, epsilon 0.6888401829382443, time 726.0, rides 136\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 94, reward 256.0, memory_length 2000, epsilon 0.6860848222064914, time 726.0, rides 137\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 95, reward -44.0, memory_length 2000, epsilon 0.6833404829176654, time 726.0, rides 129\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 96, reward 144.0, memory_length 2000, epsilon 0.6806071209859947, time 727.0, rides 123\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 97, reward -202.0, memory_length 2000, epsilon 0.6778846925020507, time 723.0, rides 129\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 98, reward 19.0, memory_length 2000, epsilon 0.6751731537320426, time 726.0, rides 139\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 99, reward 90.0, memory_length 2000, epsilon 0.6724724611171143, time 740.0, rides 121\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 100, reward -214.0, memory_length 2000, epsilon 0.6697825712726458, time 733.0, rides 124\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 101, reward -360.0, memory_length 2000, epsilon 0.6671034409875554, time 730.0, rides 120\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 102, reward 192.0, memory_length 2000, epsilon 0.664435027223605, time 727.0, rides 123\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 103, reward 272.0, memory_length 2000, epsilon 0.6617772871147106, time 730.0, rides 131\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 104, reward 150.0, memory_length 2000, epsilon 0.6591301779662518, time 743.0, rides 126\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 105, reward -60.0, memory_length 2000, epsilon 0.6564936572543868, time 725.0, rides 125\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 106, reward 213.0, memory_length 2000, epsilon 0.6538676826253692, time 731.0, rides 125\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 107, reward 213.0, memory_length 2000, epsilon 0.6512522118948678, time 733.0, rides 135\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 108, reward 116.0, memory_length 2000, epsilon 0.6486472030472883, time 731.0, rides 125\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 109, reward 113.0, memory_length 2000, epsilon 0.6460526142350992, time 732.0, rides 133\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 110, reward 344.0, memory_length 2000, epsilon 0.6434684037781587, time 733.0, rides 119\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 111, reward 48.0, memory_length 2000, epsilon 0.6408945301630461, time 723.0, rides 122\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 112, reward 221.0, memory_length 2000, epsilon 0.6383309520423939, time 722.0, rides 119\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 113, reward 21.0, memory_length 2000, epsilon 0.6357776282342243, time 724.0, rides 125\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 114, reward 83.0, memory_length 2000, epsilon 0.6332345177212875, time 727.0, rides 129\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 115, reward 89.0, memory_length 2000, epsilon 0.6307015796504023, time 742.0, rides 127\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 116, reward 360.0, memory_length 2000, epsilon 0.6281787733318007, time 726.0, rides 124\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 117, reward -174.0, memory_length 2000, epsilon 0.6256660582384734, time 726.0, rides 110\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 118, reward 312.0, memory_length 2000, epsilon 0.6231633940055196, time 721.0, rides 133\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 119, reward 264.0, memory_length 2000, epsilon 0.6206707404294975, time 727.0, rides 131\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 120, reward 318.0, memory_length 2000, epsilon 0.6181880574677795, time 729.0, rides 134\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 121, reward 39.0, memory_length 2000, epsilon 0.6157153052379084, time 731.0, rides 125\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 122, reward 180.0, memory_length 2000, epsilon 0.6132524440169568, time 728.0, rides 128\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 123, reward 30.0, memory_length 2000, epsilon 0.6107994342408889, time 734.0, rides 133\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 124, reward 91.0, memory_length 2000, epsilon 0.6083562365039253, time 730.0, rides 119\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 125, reward 277.0, memory_length 2000, epsilon 0.6059228115579096, time 734.0, rides 119\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 126, reward 103.0, memory_length 2000, epsilon 0.6034991203116781, time 746.0, rides 127\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 127, reward -56.0, memory_length 2000, epsilon 0.6010851238304313, time 732.0, rides 115\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 128, reward 224.0, memory_length 2000, epsilon 0.5986807833351095, time 727.0, rides 125\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 129, reward 198.0, memory_length 2000, epsilon 0.5962860602017691, time 730.0, rides 137\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 130, reward 108.0, memory_length 2000, epsilon 0.5939009159609621, time 730.0, rides 131\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 131, reward 117.0, memory_length 2000, epsilon 0.5915253122971182, time 724.0, rides 128\n",
      "Initial State is  [3, 21, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 132, reward 163.0, memory_length 2000, epsilon 0.5891592110479298, time 730.0, rides 123\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 133, reward -74.0, memory_length 2000, epsilon 0.586802574203738, time 733.0, rides 123\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 134, reward 320.0, memory_length 2000, epsilon 0.5844553639069231, time 731.0, rides 141\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 135, reward 271.0, memory_length 2000, epsilon 0.5821175424512953, time 742.0, rides 135\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 136, reward 128.0, memory_length 2000, epsilon 0.5797890722814902, time 728.0, rides 132\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 137, reward 136.0, memory_length 2000, epsilon 0.5774699159923642, time 728.0, rides 134\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 138, reward 137.0, memory_length 2000, epsilon 0.5751600363283947, time 732.0, rides 123\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 139, reward 242.0, memory_length 2000, epsilon 0.5728593961830811, time 732.0, rides 132\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 140, reward 444.0, memory_length 2000, epsilon 0.5705679585983489, time 723.0, rides 138\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 141, reward 493.0, memory_length 2000, epsilon 0.5682856867639554, time 723.0, rides 123\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 142, reward 135.0, memory_length 2000, epsilon 0.5660125440168996, time 729.0, rides 129\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 143, reward 248.0, memory_length 2000, epsilon 0.563748493840832, time 724.0, rides 124\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 144, reward -117.0, memory_length 2000, epsilon 0.5614934998654687, time 729.0, rides 134\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 145, reward -94.0, memory_length 2000, epsilon 0.5592475258660068, time 731.0, rides 133\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 146, reward 150.0, memory_length 2000, epsilon 0.5570105357625428, time 729.0, rides 129\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 147, reward 148.0, memory_length 2000, epsilon 0.5547824936194926, time 726.0, rides 138\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 148, reward 75.0, memory_length 2000, epsilon 0.5525633636450147, time 724.0, rides 127\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 149, reward 120.0, memory_length 2000, epsilon 0.5503531101904345, time 727.0, rides 113\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 150, reward 141.0, memory_length 2000, epsilon 0.5481516977496729, time 727.0, rides 135\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 151, reward 60.0, memory_length 2000, epsilon 0.5459590909586741, time 726.0, rides 125\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 152, reward 118.0, memory_length 2000, epsilon 0.5437752545948394, time 732.0, rides 120\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 153, reward 231.0, memory_length 2000, epsilon 0.5416001535764601, time 722.0, rides 125\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 154, reward 214.0, memory_length 2000, epsilon 0.5394337529621542, time 731.0, rides 129\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 155, reward 119.0, memory_length 2000, epsilon 0.5372760179503057, time 727.0, rides 126\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 156, reward 75.0, memory_length 2000, epsilon 0.5351269138785044, time 725.0, rides 133\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 157, reward 165.0, memory_length 2000, epsilon 0.5329864062229904, time 726.0, rides 125\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 158, reward 116.0, memory_length 2000, epsilon 0.5308544605980984, time 726.0, rides 128\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 159, reward 240.0, memory_length 2000, epsilon 0.528731042755706, time 733.0, rides 135\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 160, reward 342.0, memory_length 2000, epsilon 0.5266161185846832, time 734.0, rides 122\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 161, reward 75.0, memory_length 2000, epsilon 0.5245096541103444, time 728.0, rides 120\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 162, reward 380.0, memory_length 2000, epsilon 0.5224116154939031, time 727.0, rides 129\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 163, reward 126.0, memory_length 2000, epsilon 0.5203219690319275, time 723.0, rides 115\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 164, reward 365.0, memory_length 2000, epsilon 0.5182406811557998, time 725.0, rides 127\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 165, reward 277.0, memory_length 2000, epsilon 0.5161677184311766, time 730.0, rides 120\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 166, reward 460.0, memory_length 2000, epsilon 0.5141030475574518, time 723.0, rides 115\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 167, reward 158.0, memory_length 2000, epsilon 0.5120466353672221, time 730.0, rides 114\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 168, reward 262.0, memory_length 2000, epsilon 0.5099984488257532, time 724.0, rides 140\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 169, reward 481.0, memory_length 2000, epsilon 0.5079584550304501, time 737.0, rides 132\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 170, reward 444.0, memory_length 2000, epsilon 0.5059266212103284, time 729.0, rides 115\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 171, reward 290.0, memory_length 2000, epsilon 0.503902914725487, time 726.0, rides 130\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 172, reward 111.0, memory_length 2000, epsilon 0.501887303066585, time 729.0, rides 120\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 173, reward 566.0, memory_length 2000, epsilon 0.4998797538543187, time 730.0, rides 136\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 174, reward 149.0, memory_length 2000, epsilon 0.4978802348389015, time 734.0, rides 122\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 175, reward 164.0, memory_length 2000, epsilon 0.49588871389954586, time 732.0, rides 119\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 176, reward 326.0, memory_length 2000, epsilon 0.49390515904394766, time 728.0, rides 123\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 177, reward 301.0, memory_length 2000, epsilon 0.49192953840777187, time 728.0, rides 119\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 178, reward 115.0, memory_length 2000, epsilon 0.4899618202541408, time 722.0, rides 109\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 179, reward 324.0, memory_length 2000, epsilon 0.4880019729731242, time 724.0, rides 128\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 180, reward 83.0, memory_length 2000, epsilon 0.4860499650812317, time 731.0, rides 127\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 181, reward 91.0, memory_length 2000, epsilon 0.4841057652209068, time 735.0, rides 129\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 182, reward 484.0, memory_length 2000, epsilon 0.48216934216002316, time 724.0, rides 141\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 183, reward 589.0, memory_length 2000, epsilon 0.48024066479138305, time 739.0, rides 134\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 184, reward 273.0, memory_length 2000, epsilon 0.47831970213221753, time 739.0, rides 131\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 185, reward 384.0, memory_length 2000, epsilon 0.47640642332368865, time 735.0, rides 136\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 186, reward 272.0, memory_length 2000, epsilon 0.47450079763039393, time 728.0, rides 126\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 187, reward 201.0, memory_length 2000, epsilon 0.47260279443987235, time 725.0, rides 134\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 188, reward 335.0, memory_length 2000, epsilon 0.47071238326211284, time 733.0, rides 128\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 189, reward 315.0, memory_length 2000, epsilon 0.4688295337290644, time 729.0, rides 117\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 190, reward 318.0, memory_length 2000, epsilon 0.4669542155941481, time 727.0, rides 133\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 191, reward 441.0, memory_length 2000, epsilon 0.46508639873177154, time 724.0, rides 141\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 192, reward 328.0, memory_length 2000, epsilon 0.46322605313684445, time 732.0, rides 116\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 193, reward 158.0, memory_length 2000, epsilon 0.46137314892429704, time 734.0, rides 127\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 194, reward 575.0, memory_length 2000, epsilon 0.4595276563285999, time 727.0, rides 123\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 195, reward 469.0, memory_length 2000, epsilon 0.45768954570328546, time 731.0, rides 118\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 196, reward 419.0, memory_length 2000, epsilon 0.4558587875204723, time 729.0, rides 133\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 197, reward 541.0, memory_length 2000, epsilon 0.45403535237039044, time 733.0, rides 133\n",
      "Initial State is  [4, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 198, reward 334.0, memory_length 2000, epsilon 0.4522192109609089, time 727.0, rides 118\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 199, reward 276.0, memory_length 2000, epsilon 0.4504103341170652, time 721.0, rides 135\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 200, reward 240.0, memory_length 2000, epsilon 0.44860869278059695, time 745.0, rides 126\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 201, reward 146.0, memory_length 2000, epsilon 0.4468142580094746, time 735.0, rides 122\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 202, reward 415.0, memory_length 2000, epsilon 0.4450270009774367, time 728.0, rides 116\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 203, reward 341.0, memory_length 2000, epsilon 0.44324689297352693, time 726.0, rides 138\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 204, reward 518.0, memory_length 2000, epsilon 0.44147390540163284, time 736.0, rides 129\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 205, reward 197.0, memory_length 2000, epsilon 0.4397080097800263, time 734.0, rides 144\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 206, reward 223.0, memory_length 2000, epsilon 0.4379491777409062, time 730.0, rides 139\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 207, reward 572.0, memory_length 2000, epsilon 0.43619738102994254, time 736.0, rides 121\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 208, reward 224.0, memory_length 2000, epsilon 0.43445259150582277, time 725.0, rides 133\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 209, reward 671.0, memory_length 2000, epsilon 0.4327147811397995, time 730.0, rides 138\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 210, reward -47.0, memory_length 2000, epsilon 0.4309839220152403, time 729.0, rides 138\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 211, reward 344.0, memory_length 2000, epsilon 0.4292599863271793, time 733.0, rides 123\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 212, reward 332.0, memory_length 2000, epsilon 0.4275429463818706, time 736.0, rides 129\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 213, reward 277.0, memory_length 2000, epsilon 0.4258327745963431, time 734.0, rides 124\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 214, reward 378.0, memory_length 2000, epsilon 0.4241294434979578, time 734.0, rides 128\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 215, reward 574.0, memory_length 2000, epsilon 0.4224329257239659, time 724.0, rides 130\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 216, reward 397.0, memory_length 2000, epsilon 0.4207431940210701, time 725.0, rides 121\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 217, reward 375.0, memory_length 2000, epsilon 0.41906022124498576, time 730.0, rides 119\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 218, reward 240.0, memory_length 2000, epsilon 0.41738398036000585, time 728.0, rides 133\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 219, reward 587.0, memory_length 2000, epsilon 0.4157144444385658, time 728.0, rides 118\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 220, reward 504.0, memory_length 2000, epsilon 0.41405158666081154, time 726.0, rides 133\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 221, reward 357.0, memory_length 2000, epsilon 0.4123953803141683, time 731.0, rides 125\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 222, reward 346.0, memory_length 2000, epsilon 0.4107457987929116, time 724.0, rides 132\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 223, reward 277.0, memory_length 2000, epsilon 0.40910281559774, time 731.0, rides 127\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 224, reward 525.0, memory_length 2000, epsilon 0.40746640433534903, time 726.0, rides 128\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 225, reward 302.0, memory_length 2000, epsilon 0.4058365387180076, time 724.0, rides 140\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 226, reward 660.0, memory_length 2000, epsilon 0.4042131925631356, time 726.0, rides 115\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 227, reward 146.0, memory_length 2000, epsilon 0.40259633979288306, time 722.0, rides 140\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 228, reward 326.0, memory_length 2000, epsilon 0.4009859544337115, time 724.0, rides 130\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 229, reward 135.0, memory_length 2000, epsilon 0.39938201061597667, time 728.0, rides 117\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 230, reward 414.0, memory_length 2000, epsilon 0.39778448257351273, time 726.0, rides 132\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 231, reward 287.0, memory_length 2000, epsilon 0.3961933446432187, time 735.0, rides 121\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 232, reward 172.0, memory_length 2000, epsilon 0.3946085712646458, time 723.0, rides 126\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 233, reward 411.0, memory_length 2000, epsilon 0.3930301369795872, time 727.0, rides 117\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 234, reward 481.0, memory_length 2000, epsilon 0.3914580164316689, time 738.0, rides 138\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 235, reward 315.0, memory_length 2000, epsilon 0.3898921843659422, time 727.0, rides 122\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 236, reward 354.0, memory_length 2000, epsilon 0.38833261562847843, time 730.0, rides 121\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 237, reward 311.0, memory_length 2000, epsilon 0.38677928516596455, time 733.0, rides 122\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 238, reward 538.0, memory_length 2000, epsilon 0.3852321680253007, time 731.0, rides 123\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 239, reward 235.0, memory_length 2000, epsilon 0.3836912393531995, time 725.0, rides 134\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 240, reward 475.0, memory_length 2000, epsilon 0.38215647439578665, time 722.0, rides 117\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 241, reward 513.0, memory_length 2000, epsilon 0.3806278484982035, time 735.0, rides 123\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 242, reward 252.0, memory_length 2000, epsilon 0.3791053371042107, time 735.0, rides 130\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 243, reward 86.0, memory_length 2000, epsilon 0.37758891575579384, time 722.0, rides 128\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 244, reward 529.0, memory_length 2000, epsilon 0.3760785600927707, time 725.0, rides 122\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 245, reward 467.0, memory_length 2000, epsilon 0.37457424585239957, time 731.0, rides 133\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 246, reward 571.0, memory_length 2000, epsilon 0.37307594886899, time 734.0, rides 124\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 247, reward 376.0, memory_length 2000, epsilon 0.37158364507351405, time 729.0, rides 117\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 248, reward 716.0, memory_length 2000, epsilon 0.37009731049322, time 721.0, rides 114\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 249, reward 269.0, memory_length 2000, epsilon 0.3686169212512471, time 726.0, rides 127\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 250, reward 512.0, memory_length 2000, epsilon 0.3671424535662421, time 725.0, rides 116\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 251, reward 412.0, memory_length 2000, epsilon 0.3656738837519771, time 741.0, rides 133\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 252, reward 431.0, memory_length 2000, epsilon 0.36421118821696924, time 725.0, rides 133\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 253, reward 543.0, memory_length 2000, epsilon 0.36275434346410135, time 726.0, rides 131\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 254, reward 298.0, memory_length 2000, epsilon 0.36130332609024496, time 726.0, rides 126\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 255, reward 784.0, memory_length 2000, epsilon 0.359858112785884, time 723.0, rides 126\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 256, reward 635.0, memory_length 2000, epsilon 0.35841868033474045, time 726.0, rides 125\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 257, reward 237.0, memory_length 2000, epsilon 0.35698500561340146, time 730.0, rides 122\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 258, reward 357.0, memory_length 2000, epsilon 0.35555706559094785, time 726.0, rides 119\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 259, reward 601.0, memory_length 2000, epsilon 0.3541348373285841, time 739.0, rides 126\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 260, reward 623.0, memory_length 2000, epsilon 0.35271829797926973, time 724.0, rides 130\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 261, reward 543.0, memory_length 2000, epsilon 0.35130742478735266, time 729.0, rides 126\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 262, reward 450.0, memory_length 2000, epsilon 0.34990219508820325, time 728.0, rides 121\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 263, reward 546.0, memory_length 2000, epsilon 0.3485025863078504, time 724.0, rides 125\n",
      "Initial State is  [4, 12, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 264, reward 433.0, memory_length 2000, epsilon 0.347108575962619, time 729.0, rides 119\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 265, reward 650.0, memory_length 2000, epsilon 0.3457201416587685, time 734.0, rides 116\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 266, reward 164.0, memory_length 2000, epsilon 0.3443372610921335, time 723.0, rides 122\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 267, reward 502.0, memory_length 2000, epsilon 0.34295991204776494, time 723.0, rides 133\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 268, reward 730.0, memory_length 2000, epsilon 0.34158807239957384, time 727.0, rides 130\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 269, reward 599.0, memory_length 2000, epsilon 0.3402217201099756, time 725.0, rides 124\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 270, reward 625.0, memory_length 2000, epsilon 0.33886083322953564, time 742.0, rides 120\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 271, reward 284.0, memory_length 2000, epsilon 0.33750538989661755, time 726.0, rides 130\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 272, reward 339.0, memory_length 2000, epsilon 0.336155368337031, time 721.0, rides 116\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 273, reward 386.0, memory_length 2000, epsilon 0.33481074686368295, time 725.0, rides 113\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 274, reward 262.0, memory_length 2000, epsilon 0.3334715038762282, time 725.0, rides 127\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 275, reward 209.0, memory_length 2000, epsilon 0.33213761786072327, time 732.0, rides 108\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 276, reward 581.0, memory_length 2000, epsilon 0.3308090673892804, time 732.0, rides 124\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 277, reward 301.0, memory_length 2000, epsilon 0.32948583111972324, time 723.0, rides 119\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 278, reward 554.0, memory_length 2000, epsilon 0.32816788779524436, time 725.0, rides 125\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 279, reward 642.0, memory_length 2000, epsilon 0.3268552162440634, time 739.0, rides 122\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 280, reward 383.0, memory_length 2000, epsilon 0.3255477953790871, time 730.0, rides 137\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 281, reward 364.0, memory_length 2000, epsilon 0.32424560419757076, time 736.0, rides 134\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 282, reward 740.0, memory_length 2000, epsilon 0.3229486217807805, time 727.0, rides 121\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 283, reward 544.0, memory_length 2000, epsilon 0.32165682729365735, time 723.0, rides 126\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 284, reward 631.0, memory_length 2000, epsilon 0.32037019998448274, time 730.0, rides 136\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 285, reward 615.0, memory_length 2000, epsilon 0.3190887191845448, time 729.0, rides 117\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 286, reward 757.0, memory_length 2000, epsilon 0.3178123643078066, time 732.0, rides 111\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 287, reward 395.0, memory_length 2000, epsilon 0.3165411148505754, time 730.0, rides 121\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 288, reward 404.0, memory_length 2000, epsilon 0.3152749503911731, time 736.0, rides 120\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 289, reward 755.0, memory_length 2000, epsilon 0.3140138505896084, time 730.0, rides 133\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 290, reward 304.0, memory_length 2000, epsilon 0.31275779518725, time 730.0, rides 127\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 291, reward 509.0, memory_length 2000, epsilon 0.31150676400650096, time 729.0, rides 121\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 292, reward 592.0, memory_length 2000, epsilon 0.31026073695047496, time 728.0, rides 135\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 293, reward 486.0, memory_length 2000, epsilon 0.30901969400267304, time 736.0, rides 133\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 294, reward 412.0, memory_length 2000, epsilon 0.30778361522666237, time 734.0, rides 129\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 295, reward 530.0, memory_length 2000, epsilon 0.3065524807657557, time 728.0, rides 126\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 296, reward 441.0, memory_length 2000, epsilon 0.3053262708426927, time 734.0, rides 125\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 297, reward 305.0, memory_length 2000, epsilon 0.30410496575932194, time 733.0, rides 119\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 298, reward 669.0, memory_length 2000, epsilon 0.3028885458962846, time 731.0, rides 137\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 299, reward 432.0, memory_length 2000, epsilon 0.3016769917126995, time 732.0, rides 116\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 300, reward 372.0, memory_length 2000, epsilon 0.3004702837458487, time 730.0, rides 121\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 301, reward 489.0, memory_length 2000, epsilon 0.2992684026108653, time 737.0, rides 129\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 302, reward 679.0, memory_length 2000, epsilon 0.2980713290004218, time 724.0, rides 126\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 303, reward 834.0, memory_length 2000, epsilon 0.29687904368442014, time 743.0, rides 112\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 304, reward 706.0, memory_length 2000, epsilon 0.2956915275096825, time 724.0, rides 119\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 305, reward 286.0, memory_length 2000, epsilon 0.2945087613996437, time 723.0, rides 133\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 306, reward 644.0, memory_length 2000, epsilon 0.29333072635404517, time 728.0, rides 135\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 307, reward 836.0, memory_length 2000, epsilon 0.292157403448629, time 727.0, rides 121\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 308, reward 618.0, memory_length 2000, epsilon 0.29098877383483446, time 729.0, rides 129\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 309, reward 392.0, memory_length 2000, epsilon 0.2898248187394951, time 731.0, rides 125\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 310, reward 967.0, memory_length 2000, epsilon 0.28866551946453717, time 722.0, rides 128\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 311, reward 550.0, memory_length 2000, epsilon 0.287510857386679, time 730.0, rides 126\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 312, reward 681.0, memory_length 2000, epsilon 0.28636081395713225, time 729.0, rides 123\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 313, reward 490.0, memory_length 2000, epsilon 0.28521537070130376, time 725.0, rides 139\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 314, reward 682.0, memory_length 2000, epsilon 0.2840745092184985, time 727.0, rides 133\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 315, reward 742.0, memory_length 2000, epsilon 0.28293821118162454, time 734.0, rides 131\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 316, reward 535.0, memory_length 2000, epsilon 0.28180645833689805, time 729.0, rides 134\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 317, reward 770.0, memory_length 2000, epsilon 0.2806792325035504, time 726.0, rides 120\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 318, reward 894.0, memory_length 2000, epsilon 0.2795565155735362, time 724.0, rides 128\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 319, reward 599.0, memory_length 2000, epsilon 0.2784382895112421, time 740.0, rides 123\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 320, reward 479.0, memory_length 2000, epsilon 0.27732453635319715, time 728.0, rides 124\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 321, reward 668.0, memory_length 2000, epsilon 0.2762152382077843, time 728.0, rides 136\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 322, reward 498.0, memory_length 2000, epsilon 0.2751103772549532, time 729.0, rides 117\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 323, reward 338.0, memory_length 2000, epsilon 0.2740099357459334, time 726.0, rides 114\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 324, reward 544.0, memory_length 2000, epsilon 0.27291389600294963, time 723.0, rides 118\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 325, reward 440.0, memory_length 2000, epsilon 0.2718222404189379, time 732.0, rides 137\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 326, reward 431.0, memory_length 2000, epsilon 0.2707349514572621, time 727.0, rides 134\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 327, reward 170.0, memory_length 2000, epsilon 0.26965201165143304, time 728.0, rides 130\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 328, reward 689.0, memory_length 2000, epsilon 0.26857340360482734, time 732.0, rides 127\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 329, reward 393.0, memory_length 2000, epsilon 0.267499109990408, time 726.0, rides 115\n",
      "Initial State is  [1, 1, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 330, reward 659.0, memory_length 2000, epsilon 0.26642911355044635, time 733.0, rides 129\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 331, reward 746.0, memory_length 2000, epsilon 0.26536339709624457, time 729.0, rides 136\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 332, reward 629.0, memory_length 2000, epsilon 0.2643019435078596, time 721.0, rides 124\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 333, reward 800.0, memory_length 2000, epsilon 0.2632447357338282, time 731.0, rides 126\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 334, reward 623.0, memory_length 2000, epsilon 0.26219175679089285, time 731.0, rides 126\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 335, reward 249.0, memory_length 2000, epsilon 0.2611429897637293, time 730.0, rides 124\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 336, reward 620.0, memory_length 2000, epsilon 0.26009841780467435, time 736.0, rides 117\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 337, reward 294.0, memory_length 2000, epsilon 0.2590580241334557, time 726.0, rides 117\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 338, reward 513.0, memory_length 2000, epsilon 0.2580217920369218, time 728.0, rides 135\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 339, reward 790.0, memory_length 2000, epsilon 0.25698970486877415, time 727.0, rides 113\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 340, reward 649.0, memory_length 2000, epsilon 0.25596174604929905, time 728.0, rides 121\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 341, reward 398.0, memory_length 2000, epsilon 0.25493789906510184, time 728.0, rides 119\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 342, reward 686.0, memory_length 2000, epsilon 0.25391814746884145, time 727.0, rides 128\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 343, reward 883.0, memory_length 2000, epsilon 0.2529024748789661, time 733.0, rides 125\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 344, reward 760.0, memory_length 2000, epsilon 0.25189086497945024, time 721.0, rides 121\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 345, reward 477.0, memory_length 2000, epsilon 0.2508833015195324, time 735.0, rides 135\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 346, reward 672.0, memory_length 2000, epsilon 0.2498797683134543, time 734.0, rides 118\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 347, reward 710.0, memory_length 2000, epsilon 0.24888024924020047, time 735.0, rides 139\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 348, reward 744.0, memory_length 2000, epsilon 0.24788472824323968, time 733.0, rides 122\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 349, reward 808.0, memory_length 2000, epsilon 0.24689318933026672, time 738.0, rides 121\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 350, reward 397.0, memory_length 2000, epsilon 0.24590561657294563, time 732.0, rides 119\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 351, reward 566.0, memory_length 2000, epsilon 0.24492199410665386, time 726.0, rides 135\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 352, reward 594.0, memory_length 2000, epsilon 0.24394230613022724, time 736.0, rides 149\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 353, reward 749.0, memory_length 2000, epsilon 0.24296653690570633, time 728.0, rides 119\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 354, reward 646.0, memory_length 2000, epsilon 0.2419946707580835, time 720.0, rides 125\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 355, reward 653.0, memory_length 2000, epsilon 0.24102669207505117, time 730.0, rides 135\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 356, reward 797.0, memory_length 2000, epsilon 0.24006258530675095, time 726.0, rides 123\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 357, reward 717.0, memory_length 2000, epsilon 0.23910233496552397, time 724.0, rides 120\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 358, reward 206.0, memory_length 2000, epsilon 0.23814592562566186, time 724.0, rides 118\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 359, reward 708.0, memory_length 2000, epsilon 0.23719334192315922, time 728.0, rides 135\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 360, reward 385.0, memory_length 2000, epsilon 0.23624456855546658, time 735.0, rides 131\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 361, reward 720.0, memory_length 2000, epsilon 0.2352995902812447, time 729.0, rides 128\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 362, reward 369.0, memory_length 2000, epsilon 0.23435839192011973, time 734.0, rides 126\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 363, reward 719.0, memory_length 2000, epsilon 0.23342095835243926, time 729.0, rides 121\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 364, reward 98.0, memory_length 2000, epsilon 0.23248727451902948, time 734.0, rides 117\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 365, reward 813.0, memory_length 2000, epsilon 0.23155732542095336, time 729.0, rides 135\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 366, reward 796.0, memory_length 2000, epsilon 0.23063109611926955, time 733.0, rides 132\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 367, reward 404.0, memory_length 2000, epsilon 0.22970857173479248, time 731.0, rides 129\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 368, reward 483.0, memory_length 2000, epsilon 0.2287897374478533, time 735.0, rides 117\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 369, reward 661.0, memory_length 2000, epsilon 0.2278745784980619, time 735.0, rides 123\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 370, reward 481.0, memory_length 2000, epsilon 0.22696308018406963, time 730.0, rides 125\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 371, reward 690.0, memory_length 2000, epsilon 0.22605522786333337, time 736.0, rides 140\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 372, reward 337.0, memory_length 2000, epsilon 0.22515100695188003, time 728.0, rides 126\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 373, reward 375.0, memory_length 2000, epsilon 0.2242504029240725, time 730.0, rides 124\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 374, reward 327.0, memory_length 2000, epsilon 0.22335340131237622, time 732.0, rides 137\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 375, reward 899.0, memory_length 2000, epsilon 0.22245998770712672, time 729.0, rides 110\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 376, reward 652.0, memory_length 2000, epsilon 0.2215701477562982, time 724.0, rides 120\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 377, reward 761.0, memory_length 2000, epsilon 0.22068386716527302, time 722.0, rides 124\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 378, reward 685.0, memory_length 2000, epsilon 0.21980113169661192, time 730.0, rides 119\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 379, reward 688.0, memory_length 2000, epsilon 0.21892192716982548, time 730.0, rides 114\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 380, reward 802.0, memory_length 2000, epsilon 0.21804623946114618, time 727.0, rides 115\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 381, reward 628.0, memory_length 2000, epsilon 0.21717405450330157, time 732.0, rides 125\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 382, reward 783.0, memory_length 2000, epsilon 0.21630535828528838, time 726.0, rides 129\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 383, reward 716.0, memory_length 2000, epsilon 0.21544013685214722, time 729.0, rides 119\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 384, reward 734.0, memory_length 2000, epsilon 0.21457837630473864, time 729.0, rides 118\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 385, reward 585.0, memory_length 2000, epsilon 0.2137200627995197, time 724.0, rides 129\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 386, reward 828.0, memory_length 2000, epsilon 0.2128651825483216, time 732.0, rides 124\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 387, reward 777.0, memory_length 2000, epsilon 0.2120137218181283, time 727.0, rides 116\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 388, reward 890.0, memory_length 2000, epsilon 0.21116566693085578, time 727.0, rides 113\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 389, reward 622.0, memory_length 2000, epsilon 0.21032100426313238, time 730.0, rides 135\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 390, reward 519.0, memory_length 2000, epsilon 0.20947972024607983, time 734.0, rides 127\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 391, reward 365.0, memory_length 2000, epsilon 0.20864180136509552, time 726.0, rides 123\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 392, reward 556.0, memory_length 2000, epsilon 0.20780723415963515, time 727.0, rides 126\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 393, reward 557.0, memory_length 2000, epsilon 0.2069760052229966, time 729.0, rides 131\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 394, reward 876.0, memory_length 2000, epsilon 0.2061481012021046, time 724.0, rides 126\n",
      "Initial State is  [3, 18, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 395, reward 827.0, memory_length 2000, epsilon 0.2053235087972962, time 726.0, rides 112\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 396, reward 667.0, memory_length 2000, epsilon 0.204502214762107, time 721.0, rides 116\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 397, reward 757.0, memory_length 2000, epsilon 0.20368420590305858, time 731.0, rides 119\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 398, reward 409.0, memory_length 2000, epsilon 0.20286946907944634, time 727.0, rides 119\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 399, reward 609.0, memory_length 2000, epsilon 0.20205799120312856, time 727.0, rides 132\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 400, reward 540.0, memory_length 2000, epsilon 0.20124975923831603, time 729.0, rides 122\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 401, reward 943.0, memory_length 2000, epsilon 0.2004447602013628, time 729.0, rides 128\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 402, reward 620.0, memory_length 2000, epsilon 0.19964298116055731, time 735.0, rides 121\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 403, reward 553.0, memory_length 2000, epsilon 0.1988444092359151, time 728.0, rides 117\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 404, reward 976.0, memory_length 2000, epsilon 0.19804903159897144, time 725.0, rides 134\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 405, reward 955.0, memory_length 2000, epsilon 0.19725683547257555, time 732.0, rides 129\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 406, reward 613.0, memory_length 2000, epsilon 0.19646780813068523, time 729.0, rides 122\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 407, reward 804.0, memory_length 2000, epsilon 0.1956819368981625, time 725.0, rides 127\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 408, reward 811.0, memory_length 2000, epsilon 0.19489920915056985, time 729.0, rides 124\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 409, reward 767.0, memory_length 2000, epsilon 0.19411961231396757, time 724.0, rides 130\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 410, reward 759.0, memory_length 2000, epsilon 0.1933431338647117, time 727.0, rides 132\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 411, reward 1030.0, memory_length 2000, epsilon 0.19256976132925285, time 734.0, rides 133\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 412, reward 902.0, memory_length 2000, epsilon 0.19179948228393584, time 728.0, rides 126\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 413, reward 601.0, memory_length 2000, epsilon 0.1910322843548001, time 727.0, rides 130\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 414, reward 828.0, memory_length 2000, epsilon 0.19026815521738089, time 730.0, rides 115\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 415, reward 914.0, memory_length 2000, epsilon 0.18950708259651136, time 733.0, rides 131\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 416, reward 476.0, memory_length 2000, epsilon 0.18874905426612532, time 732.0, rides 140\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 417, reward 670.0, memory_length 2000, epsilon 0.18799405804906083, time 730.0, rides 130\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 418, reward 699.0, memory_length 2000, epsilon 0.18724208181686458, time 731.0, rides 118\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 419, reward 591.0, memory_length 2000, epsilon 0.18649311348959713, time 725.0, rides 124\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 420, reward 1054.0, memory_length 2000, epsilon 0.18574714103563872, time 732.0, rides 118\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 421, reward 693.0, memory_length 2000, epsilon 0.18500415247149618, time 732.0, rides 122\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 422, reward 721.0, memory_length 2000, epsilon 0.1842641358616102, time 727.0, rides 128\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 423, reward 527.0, memory_length 2000, epsilon 0.18352707931816375, time 733.0, rides 135\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 424, reward 319.0, memory_length 2000, epsilon 0.1827929710008911, time 735.0, rides 126\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 425, reward 305.0, memory_length 2000, epsilon 0.18206179911688752, time 723.0, rides 120\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 426, reward 597.0, memory_length 2000, epsilon 0.18133355192041997, time 723.0, rides 112\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 427, reward 797.0, memory_length 2000, epsilon 0.1806082177127383, time 730.0, rides 118\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 428, reward 736.0, memory_length 2000, epsilon 0.17988578484188733, time 723.0, rides 113\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 429, reward 657.0, memory_length 2000, epsilon 0.1791662417025198, time 726.0, rides 113\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 430, reward 584.0, memory_length 2000, epsilon 0.17844957673570971, time 732.0, rides 115\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 431, reward 925.0, memory_length 2000, epsilon 0.17773577842876687, time 735.0, rides 133\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 432, reward 528.0, memory_length 2000, epsilon 0.1770248353150518, time 735.0, rides 132\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 433, reward 643.0, memory_length 2000, epsilon 0.1763167359737916, time 725.0, rides 119\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 434, reward 669.0, memory_length 2000, epsilon 0.17561146902989644, time 733.0, rides 124\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 435, reward 962.0, memory_length 2000, epsilon 0.17490902315377685, time 727.0, rides 129\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 436, reward 914.0, memory_length 2000, epsilon 0.17420938706116174, time 738.0, rides 121\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 437, reward 668.0, memory_length 2000, epsilon 0.17351254951291709, time 726.0, rides 120\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 438, reward 708.0, memory_length 2000, epsilon 0.17281849931486543, time 721.0, rides 121\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 439, reward 938.0, memory_length 2000, epsilon 0.17212722531760596, time 726.0, rides 123\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 440, reward 652.0, memory_length 2000, epsilon 0.17143871641633554, time 729.0, rides 119\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 441, reward 763.0, memory_length 2000, epsilon 0.17075296155067018, time 729.0, rides 125\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 442, reward 966.0, memory_length 2000, epsilon 0.1700699497044675, time 736.0, rides 125\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 443, reward 885.0, memory_length 2000, epsilon 0.16938966990564963, time 731.0, rides 126\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 444, reward 694.0, memory_length 2000, epsilon 0.16871211122602703, time 736.0, rides 128\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 445, reward 951.0, memory_length 2000, epsilon 0.16803726278112294, time 720.0, rides 111\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 446, reward 1002.0, memory_length 2000, epsilon 0.16736511372999843, time 738.0, rides 113\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 447, reward 719.0, memory_length 2000, epsilon 0.16669565327507843, time 727.0, rides 110\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 448, reward 818.0, memory_length 2000, epsilon 0.16602887066197813, time 726.0, rides 120\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 449, reward 746.0, memory_length 2000, epsilon 0.16536475517933022, time 732.0, rides 122\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 450, reward 601.0, memory_length 2000, epsilon 0.1647032961586129, time 733.0, rides 139\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 451, reward 875.0, memory_length 2000, epsilon 0.16404448297397844, time 732.0, rides 131\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 452, reward 749.0, memory_length 2000, epsilon 0.16338830504208252, time 734.0, rides 122\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 453, reward 662.0, memory_length 2000, epsilon 0.1627347518219142, time 734.0, rides 124\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 454, reward 890.0, memory_length 2000, epsilon 0.16208381281462655, time 729.0, rides 124\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 455, reward 953.0, memory_length 2000, epsilon 0.16143547756336804, time 733.0, rides 124\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 456, reward 817.0, memory_length 2000, epsilon 0.16078973565311455, time 727.0, rides 130\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 457, reward 809.0, memory_length 2000, epsilon 0.16014657671050211, time 732.0, rides 138\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 458, reward 932.0, memory_length 2000, epsilon 0.1595059904036601, time 725.0, rides 131\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 459, reward 857.0, memory_length 2000, epsilon 0.15886796644204546, time 723.0, rides 119\n",
      "Initial State is  [2, 0, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 460, reward 1127.0, memory_length 2000, epsilon 0.15823249457627728, time 728.0, rides 129\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 461, reward 787.0, memory_length 2000, epsilon 0.15759956459797217, time 732.0, rides 137\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 462, reward 492.0, memory_length 2000, epsilon 0.15696916633958027, time 723.0, rides 125\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 463, reward 891.0, memory_length 2000, epsilon 0.15634128967422195, time 728.0, rides 124\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 464, reward 908.0, memory_length 2000, epsilon 0.15571592451552507, time 725.0, rides 118\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 465, reward 802.0, memory_length 2000, epsilon 0.15509306081746296, time 724.0, rides 122\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 466, reward 834.0, memory_length 2000, epsilon 0.1544726885741931, time 726.0, rides 128\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 467, reward 921.0, memory_length 2000, epsilon 0.15385479781989633, time 727.0, rides 125\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 468, reward 882.0, memory_length 2000, epsilon 0.15323937862861675, time 728.0, rides 139\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 469, reward 780.0, memory_length 2000, epsilon 0.1526264211141023, time 726.0, rides 120\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 470, reward 563.0, memory_length 2000, epsilon 0.15201591542964588, time 724.0, rides 121\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 471, reward 1204.0, memory_length 2000, epsilon 0.1514078517679273, time 724.0, rides 122\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 472, reward 620.0, memory_length 2000, epsilon 0.15080222036085558, time 735.0, rides 127\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 473, reward 1091.0, memory_length 2000, epsilon 0.15019901147941214, time 733.0, rides 125\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 474, reward 767.0, memory_length 2000, epsilon 0.1495982154334945, time 734.0, rides 132\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 475, reward 996.0, memory_length 2000, epsilon 0.14899982257176053, time 733.0, rides 114\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 476, reward 1109.0, memory_length 2000, epsilon 0.14840382328147347, time 721.0, rides 121\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 477, reward 1010.0, memory_length 2000, epsilon 0.1478102079883476, time 732.0, rides 112\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 478, reward 806.0, memory_length 2000, epsilon 0.1472189671563942, time 723.0, rides 119\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 479, reward 911.0, memory_length 2000, epsilon 0.14663009128776863, time 728.0, rides 118\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 480, reward 786.0, memory_length 2000, epsilon 0.14604357092261755, time 731.0, rides 124\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 481, reward 1008.0, memory_length 2000, epsilon 0.14545939663892707, time 722.0, rides 121\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 482, reward 731.0, memory_length 2000, epsilon 0.14487755905237137, time 726.0, rides 119\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 483, reward 1003.0, memory_length 2000, epsilon 0.14429804881616187, time 726.0, rides 127\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 484, reward 779.0, memory_length 2000, epsilon 0.14372085662089723, time 725.0, rides 131\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 485, reward 974.0, memory_length 2000, epsilon 0.14314597319441363, time 728.0, rides 127\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 486, reward 960.0, memory_length 2000, epsilon 0.142573389301636, time 732.0, rides 132\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 487, reward 766.0, memory_length 2000, epsilon 0.14200309574442946, time 733.0, rides 136\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 488, reward 713.0, memory_length 2000, epsilon 0.14143508336145172, time 734.0, rides 118\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 489, reward 826.0, memory_length 2000, epsilon 0.1408693430280059, time 723.0, rides 117\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 490, reward 659.0, memory_length 2000, epsilon 0.1403058656558939, time 735.0, rides 120\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 491, reward 1103.0, memory_length 2000, epsilon 0.13974464219327032, time 729.0, rides 126\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 492, reward 738.0, memory_length 2000, epsilon 0.13918566362449725, time 720.0, rides 120\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 493, reward 1020.0, memory_length 2000, epsilon 0.13862892096999924, time 729.0, rides 122\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 494, reward 1038.0, memory_length 2000, epsilon 0.13807440528611925, time 731.0, rides 125\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 495, reward 453.0, memory_length 2000, epsilon 0.13752210766497477, time 727.0, rides 116\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 496, reward 850.0, memory_length 2000, epsilon 0.13697201923431487, time 726.0, rides 119\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 497, reward 836.0, memory_length 2000, epsilon 0.13642413115737761, time 730.0, rides 121\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 498, reward 738.0, memory_length 2000, epsilon 0.1358784346327481, time 731.0, rides 127\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 499, reward 653.0, memory_length 2000, epsilon 0.13533492089421711, time 731.0, rides 119\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 500, reward 855.0, memory_length 2000, epsilon 0.13479358121064025, time 729.0, rides 115\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 501, reward 787.0, memory_length 2000, epsilon 0.13425440688579768, time 728.0, rides 113\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 502, reward 837.0, memory_length 2000, epsilon 0.1337173892582545, time 721.0, rides 131\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 503, reward 886.0, memory_length 2000, epsilon 0.13318251970122147, time 725.0, rides 119\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 504, reward 699.0, memory_length 2000, epsilon 0.13264978962241658, time 724.0, rides 121\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 505, reward 711.0, memory_length 2000, epsilon 0.13211919046392692, time 728.0, rides 115\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 506, reward 932.0, memory_length 2000, epsilon 0.1315907137020712, time 732.0, rides 127\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 507, reward 849.0, memory_length 2000, epsilon 0.13106435084726292, time 736.0, rides 119\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 508, reward 644.0, memory_length 2000, epsilon 0.13054009344387388, time 732.0, rides 112\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 509, reward 778.0, memory_length 2000, epsilon 0.13001793307009837, time 735.0, rides 120\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 510, reward 882.0, memory_length 2000, epsilon 0.129497861337818, time 727.0, rides 126\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 511, reward 674.0, memory_length 2000, epsilon 0.1289798698924667, time 733.0, rides 127\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 512, reward 929.0, memory_length 2000, epsilon 0.12846395041289685, time 729.0, rides 127\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 513, reward 912.0, memory_length 2000, epsilon 0.12795009461124526, time 728.0, rides 115\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 514, reward 578.0, memory_length 2000, epsilon 0.12743829423280026, time 734.0, rides 123\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 515, reward 920.0, memory_length 2000, epsilon 0.12692854105586907, time 729.0, rides 118\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 516, reward 406.0, memory_length 2000, epsilon 0.1264208268916456, time 726.0, rides 124\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 517, reward 762.0, memory_length 2000, epsilon 0.12591514358407901, time 730.0, rides 111\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 518, reward 649.0, memory_length 2000, epsilon 0.1254114830097427, time 731.0, rides 123\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 519, reward 1162.0, memory_length 2000, epsilon 0.12490983707770373, time 725.0, rides 127\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 520, reward 728.0, memory_length 2000, epsilon 0.12441019772939291, time 730.0, rides 126\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 521, reward 728.0, memory_length 2000, epsilon 0.12391255693847533, time 737.0, rides 119\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 522, reward 521.0, memory_length 2000, epsilon 0.12341690671072143, time 727.0, rides 126\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 523, reward 698.0, memory_length 2000, epsilon 0.12292323908387855, time 723.0, rides 118\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 524, reward 485.0, memory_length 2000, epsilon 0.12243154612754303, time 743.0, rides 126\n",
      "Initial State is  [0, 4, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 525, reward 936.0, memory_length 2000, epsilon 0.12194181994303287, time 732.0, rides 117\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 526, reward 798.0, memory_length 2000, epsilon 0.12145405266326073, time 730.0, rides 118\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 527, reward 675.0, memory_length 2000, epsilon 0.12096823645260768, time 736.0, rides 124\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 528, reward 851.0, memory_length 2000, epsilon 0.12048436350679725, time 736.0, rides 124\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 529, reward 845.0, memory_length 2000, epsilon 0.12000242605277006, time 732.0, rides 120\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 530, reward 603.0, memory_length 2000, epsilon 0.11952241634855898, time 730.0, rides 117\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 531, reward 1029.0, memory_length 2000, epsilon 0.11904432668316475, time 728.0, rides 122\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 532, reward 1015.0, memory_length 2000, epsilon 0.11856814937643209, time 734.0, rides 127\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 533, reward 592.0, memory_length 2000, epsilon 0.11809387677892635, time 722.0, rides 113\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 534, reward 878.0, memory_length 2000, epsilon 0.11762150127181065, time 737.0, rides 121\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 535, reward 931.0, memory_length 2000, epsilon 0.1171510152667234, time 727.0, rides 121\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 536, reward 1062.0, memory_length 2000, epsilon 0.11668241120565652, time 730.0, rides 131\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 537, reward 811.0, memory_length 2000, epsilon 0.1162156815608339, time 731.0, rides 130\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 538, reward 775.0, memory_length 2000, epsilon 0.11575081883459055, time 735.0, rides 118\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 539, reward 760.0, memory_length 2000, epsilon 0.11528781555925219, time 722.0, rides 136\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 540, reward 529.0, memory_length 2000, epsilon 0.11482666429701519, time 738.0, rides 143\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 541, reward 827.0, memory_length 2000, epsilon 0.11436735763982712, time 732.0, rides 111\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 542, reward 757.0, memory_length 2000, epsilon 0.11390988820926781, time 728.0, rides 114\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 543, reward 1167.0, memory_length 2000, epsilon 0.11345424865643074, time 728.0, rides 132\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 544, reward 786.0, memory_length 2000, epsilon 0.11300043166180501, time 731.0, rides 123\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 545, reward 1256.0, memory_length 2000, epsilon 0.11254842993515779, time 724.0, rides 132\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 546, reward 865.0, memory_length 2000, epsilon 0.11209823621541716, time 733.0, rides 116\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 547, reward 933.0, memory_length 2000, epsilon 0.1116498432705555, time 736.0, rides 123\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 548, reward 628.0, memory_length 2000, epsilon 0.11120324389747327, time 734.0, rides 127\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 549, reward 835.0, memory_length 2000, epsilon 0.11075843092188338, time 732.0, rides 122\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 550, reward 786.0, memory_length 2000, epsilon 0.11031539719819584, time 728.0, rides 113\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 551, reward 1029.0, memory_length 2000, epsilon 0.10987413560940307, time 730.0, rides 112\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 552, reward 1060.0, memory_length 2000, epsilon 0.10943463906696545, time 727.0, rides 121\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 553, reward 1201.0, memory_length 2000, epsilon 0.10899690051069759, time 726.0, rides 125\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 554, reward 858.0, memory_length 2000, epsilon 0.1085609129086548, time 722.0, rides 113\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 555, reward 940.0, memory_length 2000, epsilon 0.10812666925702018, time 732.0, rides 133\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 556, reward 752.0, memory_length 2000, epsilon 0.10769416257999209, time 729.0, rides 115\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 557, reward 866.0, memory_length 2000, epsilon 0.10726338592967213, time 727.0, rides 118\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 558, reward 1010.0, memory_length 2000, epsilon 0.10683433238595344, time 741.0, rides 121\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 559, reward 840.0, memory_length 2000, epsilon 0.10640699505640963, time 722.0, rides 123\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 560, reward 1054.0, memory_length 2000, epsilon 0.10598136707618398, time 728.0, rides 115\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 561, reward 736.0, memory_length 2000, epsilon 0.10555744160787925, time 734.0, rides 136\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 562, reward 728.0, memory_length 2000, epsilon 0.10513521184144774, time 728.0, rides 117\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 563, reward 945.0, memory_length 2000, epsilon 0.10471467099408194, time 731.0, rides 116\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 564, reward 778.0, memory_length 2000, epsilon 0.10429581231010561, time 725.0, rides 119\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 565, reward 917.0, memory_length 2000, epsilon 0.10387862906086519, time 727.0, rides 125\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 566, reward 1228.0, memory_length 2000, epsilon 0.10346311454462173, time 730.0, rides 136\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 567, reward 1021.0, memory_length 2000, epsilon 0.10304926208644324, time 725.0, rides 135\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 568, reward 781.0, memory_length 2000, epsilon 0.10263706503809747, time 735.0, rides 114\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 569, reward 1060.0, memory_length 2000, epsilon 0.10222651677794507, time 732.0, rides 118\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 570, reward 994.0, memory_length 2000, epsilon 0.1018176107108333, time 724.0, rides 121\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 571, reward 817.0, memory_length 2000, epsilon 0.10141034026798997, time 722.0, rides 123\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 572, reward 1072.0, memory_length 2000, epsilon 0.101004698906918, time 732.0, rides 135\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 573, reward 870.0, memory_length 2000, epsilon 0.10060068011129034, time 728.0, rides 121\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 574, reward 1213.0, memory_length 2000, epsilon 0.10019827739084516, time 731.0, rides 128\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 575, reward 681.0, memory_length 2000, epsilon 0.09979748428128178, time 732.0, rides 114\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 576, reward 908.0, memory_length 2000, epsilon 0.09939829434415666, time 728.0, rides 117\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 577, reward 882.0, memory_length 2000, epsilon 0.09900070116678003, time 732.0, rides 112\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 578, reward 765.0, memory_length 2000, epsilon 0.09860469836211291, time 735.0, rides 113\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 579, reward 756.0, memory_length 2000, epsilon 0.09821027956866446, time 732.0, rides 122\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 580, reward 785.0, memory_length 2000, epsilon 0.0978174384503898, time 732.0, rides 119\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 581, reward 602.0, memory_length 2000, epsilon 0.09742616869658824, time 731.0, rides 138\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 582, reward 728.0, memory_length 2000, epsilon 0.0970364640218019, time 726.0, rides 121\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 583, reward 660.0, memory_length 2000, epsilon 0.09664831816571469, time 734.0, rides 125\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 584, reward 854.0, memory_length 2000, epsilon 0.09626172489305182, time 730.0, rides 117\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 585, reward 875.0, memory_length 2000, epsilon 0.09587667799347961, time 727.0, rides 132\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 586, reward 1028.0, memory_length 2000, epsilon 0.0954931712815057, time 734.0, rides 121\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 587, reward 1003.0, memory_length 2000, epsilon 0.09511119859637968, time 730.0, rides 119\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 588, reward 925.0, memory_length 2000, epsilon 0.09473075380199415, time 725.0, rides 117\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 589, reward 833.0, memory_length 2000, epsilon 0.09435183078678618, time 727.0, rides 131\n",
      "Initial State is  [3, 20, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 590, reward 929.0, memory_length 2000, epsilon 0.09397442346363903, time 729.0, rides 122\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 591, reward 622.0, memory_length 2000, epsilon 0.09359852576978447, time 727.0, rides 126\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 592, reward 648.0, memory_length 2000, epsilon 0.09322413166670533, time 733.0, rides 121\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 593, reward 398.0, memory_length 2000, epsilon 0.09285123514003851, time 726.0, rides 122\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 594, reward 920.0, memory_length 2000, epsilon 0.09247983019947836, time 734.0, rides 136\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 595, reward 1011.0, memory_length 2000, epsilon 0.09210991087868045, time 731.0, rides 126\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 596, reward 948.0, memory_length 2000, epsilon 0.09174147123516573, time 723.0, rides 128\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 597, reward 1130.0, memory_length 2000, epsilon 0.09137450535022507, time 729.0, rides 121\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 598, reward 547.0, memory_length 2000, epsilon 0.09100900732882417, time 732.0, rides 132\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 599, reward 1009.0, memory_length 2000, epsilon 0.09064497129950887, time 721.0, rides 135\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 600, reward 881.0, memory_length 2000, epsilon 0.09028239141431083, time 731.0, rides 121\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 601, reward 884.0, memory_length 2000, epsilon 0.08992126184865358, time 725.0, rides 125\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 602, reward 862.0, memory_length 2000, epsilon 0.08956157680125897, time 731.0, rides 124\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 603, reward 1104.0, memory_length 2000, epsilon 0.08920333049405393, time 735.0, rides 120\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 604, reward 1069.0, memory_length 2000, epsilon 0.08884651717207771, time 729.0, rides 124\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 605, reward 902.0, memory_length 2000, epsilon 0.08849113110338941, time 730.0, rides 125\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 606, reward 787.0, memory_length 2000, epsilon 0.08813716657897586, time 723.0, rides 129\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 607, reward 695.0, memory_length 2000, epsilon 0.08778461791265994, time 724.0, rides 126\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 608, reward 843.0, memory_length 2000, epsilon 0.0874334794410093, time 731.0, rides 130\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 609, reward 921.0, memory_length 2000, epsilon 0.08708374552324527, time 731.0, rides 117\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 610, reward 1281.0, memory_length 2000, epsilon 0.08673541054115229, time 735.0, rides 134\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 611, reward 876.0, memory_length 2000, epsilon 0.08638846889898767, time 729.0, rides 115\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 612, reward 842.0, memory_length 2000, epsilon 0.08604291502339173, time 734.0, rides 129\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 613, reward 1079.0, memory_length 2000, epsilon 0.08569874336329816, time 729.0, rides 131\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 614, reward 944.0, memory_length 2000, epsilon 0.08535594838984496, time 724.0, rides 136\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 615, reward 884.0, memory_length 2000, epsilon 0.08501452459628558, time 731.0, rides 122\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 616, reward 1228.0, memory_length 2000, epsilon 0.08467446649790045, time 727.0, rides 122\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 617, reward 558.0, memory_length 2000, epsilon 0.08433576863190884, time 728.0, rides 123\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 618, reward 1035.0, memory_length 2000, epsilon 0.08399842555738121, time 723.0, rides 120\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 619, reward 986.0, memory_length 2000, epsilon 0.08366243185515168, time 721.0, rides 116\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 620, reward 718.0, memory_length 2000, epsilon 0.08332778212773108, time 739.0, rides 135\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 621, reward 1037.0, memory_length 2000, epsilon 0.08299447099922015, time 732.0, rides 136\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 622, reward 932.0, memory_length 2000, epsilon 0.08266249311522327, time 725.0, rides 135\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 623, reward 958.0, memory_length 2000, epsilon 0.08233184314276237, time 731.0, rides 118\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 624, reward 727.0, memory_length 2000, epsilon 0.08200251577019133, time 726.0, rides 121\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 625, reward 811.0, memory_length 2000, epsilon 0.08167450570711056, time 743.0, rides 124\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 626, reward 891.0, memory_length 2000, epsilon 0.08134780768428211, time 733.0, rides 127\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 627, reward 1003.0, memory_length 2000, epsilon 0.081022416453545, time 730.0, rides 125\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 628, reward 662.0, memory_length 2000, epsilon 0.08069832678773081, time 727.0, rides 123\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 629, reward 1000.0, memory_length 2000, epsilon 0.08037553348057988, time 735.0, rides 121\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 630, reward 900.0, memory_length 2000, epsilon 0.08005403134665756, time 731.0, rides 125\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 631, reward 815.0, memory_length 2000, epsilon 0.07973381522127093, time 731.0, rides 128\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 632, reward 776.0, memory_length 2000, epsilon 0.07941487996038585, time 730.0, rides 126\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 633, reward 789.0, memory_length 2000, epsilon 0.0790972204405443, time 722.0, rides 131\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 634, reward 871.0, memory_length 2000, epsilon 0.07878083155878213, time 732.0, rides 120\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 635, reward 952.0, memory_length 2000, epsilon 0.078465708232547, time 738.0, rides 135\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 636, reward 778.0, memory_length 2000, epsilon 0.07815184539961681, time 731.0, rides 114\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 637, reward 673.0, memory_length 2000, epsilon 0.07783923801801834, time 726.0, rides 129\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 638, reward 1002.0, memory_length 2000, epsilon 0.07752788106594627, time 732.0, rides 129\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 639, reward 1074.0, memory_length 2000, epsilon 0.07721776954168248, time 723.0, rides 128\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 640, reward 933.0, memory_length 2000, epsilon 0.07690889846351576, time 732.0, rides 123\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 641, reward 982.0, memory_length 2000, epsilon 0.0766012628696617, time 727.0, rides 119\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 642, reward 1019.0, memory_length 2000, epsilon 0.07629485781818304, time 732.0, rides 137\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 643, reward 692.0, memory_length 2000, epsilon 0.07598967838691031, time 726.0, rides 121\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 644, reward 732.0, memory_length 2000, epsilon 0.07568571967336267, time 729.0, rides 109\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 645, reward 814.0, memory_length 2000, epsilon 0.07538297679466922, time 728.0, rides 117\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 646, reward 906.0, memory_length 2000, epsilon 0.07508144488749054, time 743.0, rides 123\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 647, reward 1125.0, memory_length 2000, epsilon 0.07478111910794058, time 728.0, rides 127\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 648, reward 854.0, memory_length 2000, epsilon 0.07448199463150881, time 731.0, rides 127\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 649, reward 716.0, memory_length 2000, epsilon 0.07418406665298279, time 729.0, rides 120\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 650, reward 1049.0, memory_length 2000, epsilon 0.07388733038637085, time 728.0, rides 130\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 651, reward 798.0, memory_length 2000, epsilon 0.07359178106482536, time 734.0, rides 140\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 652, reward 1095.0, memory_length 2000, epsilon 0.07329741394056606, time 734.0, rides 126\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 653, reward 914.0, memory_length 2000, epsilon 0.0730042242848038, time 736.0, rides 137\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 654, reward 699.0, memory_length 2000, epsilon 0.07271220738766458, time 733.0, rides 132\n",
      "Initial State is  [2, 14, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 655, reward 1056.0, memory_length 2000, epsilon 0.07242135855811392, time 737.0, rides 124\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 656, reward 1059.0, memory_length 2000, epsilon 0.07213167312388147, time 730.0, rides 121\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 657, reward 991.0, memory_length 2000, epsilon 0.07184314643138595, time 730.0, rides 122\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 658, reward 1166.0, memory_length 2000, epsilon 0.0715557738456604, time 726.0, rides 124\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 659, reward 898.0, memory_length 2000, epsilon 0.07126955075027776, time 736.0, rides 119\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 660, reward 859.0, memory_length 2000, epsilon 0.07098447254727665, time 730.0, rides 133\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 661, reward 988.0, memory_length 2000, epsilon 0.07070053465708755, time 738.0, rides 124\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 662, reward 1026.0, memory_length 2000, epsilon 0.07041773251845919, time 736.0, rides 110\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 663, reward 1141.0, memory_length 2000, epsilon 0.07013606158838535, time 740.0, rides 138\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 664, reward 1017.0, memory_length 2000, epsilon 0.06985551734203181, time 729.0, rides 128\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 665, reward 870.0, memory_length 2000, epsilon 0.06957609527266369, time 733.0, rides 124\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 666, reward 1038.0, memory_length 2000, epsilon 0.06929779089157304, time 730.0, rides 127\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 667, reward 1230.0, memory_length 2000, epsilon 0.06902059972800674, time 724.0, rides 124\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 668, reward 800.0, memory_length 2000, epsilon 0.06874451732909471, time 730.0, rides 136\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 669, reward 791.0, memory_length 2000, epsilon 0.06846953925977833, time 732.0, rides 122\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 670, reward 1174.0, memory_length 2000, epsilon 0.06819566110273921, time 737.0, rides 132\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 671, reward 821.0, memory_length 2000, epsilon 0.06792287845832826, time 722.0, rides 118\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 672, reward 988.0, memory_length 2000, epsilon 0.06765118694449494, time 728.0, rides 133\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 673, reward 762.0, memory_length 2000, epsilon 0.06738058219671697, time 729.0, rides 118\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 674, reward 914.0, memory_length 2000, epsilon 0.0671110598679301, time 732.0, rides 124\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 675, reward 666.0, memory_length 2000, epsilon 0.06684261562845838, time 729.0, rides 116\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 676, reward 894.0, memory_length 2000, epsilon 0.06657524516594454, time 727.0, rides 110\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 677, reward 717.0, memory_length 2000, epsilon 0.06630894418528077, time 726.0, rides 124\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 678, reward 914.0, memory_length 2000, epsilon 0.06604370840853964, time 728.0, rides 113\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 679, reward 784.0, memory_length 2000, epsilon 0.06577953357490549, time 737.0, rides 118\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 680, reward 997.0, memory_length 2000, epsilon 0.06551641544060587, time 729.0, rides 118\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 681, reward 879.0, memory_length 2000, epsilon 0.06525434977884344, time 732.0, rides 131\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 682, reward 947.0, memory_length 2000, epsilon 0.06499333237972807, time 734.0, rides 137\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 683, reward 793.0, memory_length 2000, epsilon 0.06473335905020915, time 724.0, rides 127\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 684, reward 1076.0, memory_length 2000, epsilon 0.06447442561400832, time 726.0, rides 114\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 685, reward 894.0, memory_length 2000, epsilon 0.06421652791155229, time 729.0, rides 132\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 686, reward 1051.0, memory_length 2000, epsilon 0.06395966179990607, time 728.0, rides 123\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 687, reward 897.0, memory_length 2000, epsilon 0.06370382315270645, time 727.0, rides 129\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 688, reward 1026.0, memory_length 2000, epsilon 0.06344900786009562, time 731.0, rides 123\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 689, reward 768.0, memory_length 2000, epsilon 0.06319521182865524, time 725.0, rides 121\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 690, reward 992.0, memory_length 2000, epsilon 0.06294243098134061, time 730.0, rides 123\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 691, reward 1083.0, memory_length 2000, epsilon 0.06269066125741525, time 727.0, rides 122\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 692, reward 782.0, memory_length 2000, epsilon 0.062439898612385594, time 728.0, rides 126\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 693, reward 519.0, memory_length 2000, epsilon 0.06219013901793605, time 730.0, rides 127\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 694, reward 1139.0, memory_length 2000, epsilon 0.06194137846186431, time 725.0, rides 114\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 695, reward 1215.0, memory_length 2000, epsilon 0.06169361294801685, time 725.0, rides 124\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 696, reward 841.0, memory_length 2000, epsilon 0.06144683849622478, time 743.0, rides 112\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 697, reward 1147.0, memory_length 2000, epsilon 0.06120105114223988, time 728.0, rides 121\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 698, reward 1161.0, memory_length 2000, epsilon 0.06095624693767092, time 726.0, rides 126\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 699, reward 994.0, memory_length 2000, epsilon 0.060712421949920235, time 731.0, rides 124\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 700, reward 1297.0, memory_length 2000, epsilon 0.060469572262120554, time 730.0, rides 133\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 701, reward 640.0, memory_length 2000, epsilon 0.06022769397307207, time 728.0, rides 131\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 702, reward 814.0, memory_length 2000, epsilon 0.05998678319717979, time 727.0, rides 116\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 703, reward 828.0, memory_length 2000, epsilon 0.059746836064391066, time 724.0, rides 120\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 704, reward 1043.0, memory_length 2000, epsilon 0.059507848720133504, time 727.0, rides 121\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 705, reward 748.0, memory_length 2000, epsilon 0.059269817325252966, time 729.0, rides 119\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 706, reward 990.0, memory_length 2000, epsilon 0.05903273805595195, time 721.0, rides 119\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 707, reward 678.0, memory_length 2000, epsilon 0.058796607103728145, time 723.0, rides 115\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 708, reward 899.0, memory_length 2000, epsilon 0.05856142067531323, time 740.0, rides 121\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 709, reward 1151.0, memory_length 2000, epsilon 0.05832717499261198, time 730.0, rides 129\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 710, reward 970.0, memory_length 2000, epsilon 0.05809386629264154, time 738.0, rides 128\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 711, reward 843.0, memory_length 2000, epsilon 0.05786149082747097, time 738.0, rides 109\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 712, reward 1016.0, memory_length 2000, epsilon 0.05763004486416108, time 735.0, rides 131\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 713, reward 815.0, memory_length 2000, epsilon 0.05739952468470444, time 726.0, rides 124\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 714, reward 1131.0, memory_length 2000, epsilon 0.05716992658596562, time 729.0, rides 129\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 715, reward 740.0, memory_length 2000, epsilon 0.05694124687962176, time 733.0, rides 131\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 716, reward 1030.0, memory_length 2000, epsilon 0.05671348189210327, time 730.0, rides 129\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 717, reward 909.0, memory_length 2000, epsilon 0.05648662796453486, time 721.0, rides 124\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 718, reward 1174.0, memory_length 2000, epsilon 0.056260681452676715, time 734.0, rides 129\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 719, reward 1121.0, memory_length 2000, epsilon 0.05603563872686601, time 734.0, rides 113\n",
      "Initial State is  [1, 21, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 720, reward 920.0, memory_length 2000, epsilon 0.05581149617195855, time 725.0, rides 119\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 721, reward 914.0, memory_length 2000, epsilon 0.05558825018727071, time 732.0, rides 142\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 722, reward 956.0, memory_length 2000, epsilon 0.05536589718652163, time 733.0, rides 113\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 723, reward 842.0, memory_length 2000, epsilon 0.055144433597775544, time 725.0, rides 130\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 724, reward 983.0, memory_length 2000, epsilon 0.05492385586338444, time 729.0, rides 123\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 725, reward 955.0, memory_length 2000, epsilon 0.0547041604399309, time 731.0, rides 129\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 726, reward 1166.0, memory_length 2000, epsilon 0.05448534379817118, time 726.0, rides 124\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 727, reward 1157.0, memory_length 2000, epsilon 0.05426740242297849, time 726.0, rides 127\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 728, reward 839.0, memory_length 2000, epsilon 0.054050332813286577, time 729.0, rides 123\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 729, reward 1194.0, memory_length 2000, epsilon 0.053834131482033434, time 729.0, rides 123\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 730, reward 1089.0, memory_length 2000, epsilon 0.053618794956105297, time 725.0, rides 137\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 731, reward 817.0, memory_length 2000, epsilon 0.05340431977628088, time 732.0, rides 134\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 732, reward 884.0, memory_length 2000, epsilon 0.05319070249717575, time 733.0, rides 119\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 733, reward 912.0, memory_length 2000, epsilon 0.05297793968718705, time 731.0, rides 136\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 734, reward 788.0, memory_length 2000, epsilon 0.052766027928438305, time 728.0, rides 135\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 735, reward 693.0, memory_length 2000, epsilon 0.052554963816724545, time 726.0, rides 123\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 736, reward 976.0, memory_length 2000, epsilon 0.05234474396145765, time 730.0, rides 126\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 737, reward 888.0, memory_length 2000, epsilon 0.05213536498561182, time 731.0, rides 117\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 738, reward 934.0, memory_length 2000, epsilon 0.05192682352566937, time 733.0, rides 127\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 739, reward 870.0, memory_length 2000, epsilon 0.05171911623156669, time 738.0, rides 124\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 740, reward 1044.0, memory_length 2000, epsilon 0.051512239766640426, time 724.0, rides 126\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 741, reward 1049.0, memory_length 2000, epsilon 0.05130619080757386, time 727.0, rides 130\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 742, reward 713.0, memory_length 2000, epsilon 0.05110096604434357, time 733.0, rides 127\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 743, reward 961.0, memory_length 2000, epsilon 0.050896562180166194, time 727.0, rides 110\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 744, reward 992.0, memory_length 2000, epsilon 0.05069297593144553, time 730.0, rides 117\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 745, reward 789.0, memory_length 2000, epsilon 0.05049020402771975, time 723.0, rides 126\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 746, reward 1119.0, memory_length 2000, epsilon 0.050288243211608866, time 731.0, rides 130\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 747, reward 917.0, memory_length 2000, epsilon 0.050087090238762434, time 728.0, rides 123\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 748, reward 1221.0, memory_length 2000, epsilon 0.04988674187780738, time 722.0, rides 135\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 749, reward 1081.0, memory_length 2000, epsilon 0.049687194910296155, time 722.0, rides 128\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 750, reward 1104.0, memory_length 2000, epsilon 0.04948844613065497, time 733.0, rides 119\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 751, reward 1007.0, memory_length 2000, epsilon 0.04929049234613235, time 730.0, rides 127\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 752, reward 1193.0, memory_length 2000, epsilon 0.04909333037674782, time 726.0, rides 135\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 753, reward 678.0, memory_length 2000, epsilon 0.04889695705524083, time 722.0, rides 116\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 754, reward 1039.0, memory_length 2000, epsilon 0.04870136922701986, time 733.0, rides 129\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 755, reward 1351.0, memory_length 2000, epsilon 0.04850656375011179, time 728.0, rides 133\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 756, reward 859.0, memory_length 2000, epsilon 0.048312537495111336, time 722.0, rides 127\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 757, reward 1042.0, memory_length 2000, epsilon 0.04811928734513089, time 732.0, rides 115\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 758, reward 791.0, memory_length 2000, epsilon 0.04792681019575037, time 729.0, rides 123\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 759, reward 855.0, memory_length 2000, epsilon 0.047735102954967364, time 724.0, rides 118\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 760, reward 877.0, memory_length 2000, epsilon 0.0475441625431475, time 728.0, rides 126\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 761, reward 1065.0, memory_length 2000, epsilon 0.047353985892974904, time 735.0, rides 126\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 762, reward 1158.0, memory_length 2000, epsilon 0.047164569949403004, time 728.0, rides 121\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 763, reward 996.0, memory_length 2000, epsilon 0.046975911669605394, time 724.0, rides 119\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 764, reward 890.0, memory_length 2000, epsilon 0.04678800802292697, time 732.0, rides 126\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 765, reward 1215.0, memory_length 2000, epsilon 0.046600855990835265, time 732.0, rides 125\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 766, reward 765.0, memory_length 2000, epsilon 0.046414452566871926, time 724.0, rides 130\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 767, reward 1002.0, memory_length 2000, epsilon 0.046228794756604435, time 731.0, rides 113\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 768, reward 928.0, memory_length 2000, epsilon 0.046043879577578016, time 726.0, rides 126\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 769, reward 941.0, memory_length 2000, epsilon 0.0458597040592677, time 734.0, rides 114\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 770, reward 708.0, memory_length 2000, epsilon 0.04567626524303064, time 722.0, rides 128\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 771, reward 716.0, memory_length 2000, epsilon 0.04549356018205851, time 729.0, rides 121\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 772, reward 1168.0, memory_length 2000, epsilon 0.04531158594133028, time 730.0, rides 119\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 773, reward 1225.0, memory_length 2000, epsilon 0.045130339597564954, time 728.0, rides 120\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 774, reward 942.0, memory_length 2000, epsilon 0.044949818239174696, time 735.0, rides 123\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 775, reward 948.0, memory_length 2000, epsilon 0.044770018966217996, time 728.0, rides 125\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 776, reward 856.0, memory_length 2000, epsilon 0.04459093889035313, time 723.0, rides 117\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 777, reward 1219.0, memory_length 2000, epsilon 0.044412575134791715, time 736.0, rides 140\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 778, reward 1050.0, memory_length 2000, epsilon 0.044234924834252544, time 726.0, rides 127\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 779, reward 1129.0, memory_length 2000, epsilon 0.04405798513491554, time 730.0, rides 129\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 780, reward 1088.0, memory_length 2000, epsilon 0.04388175319437587, time 736.0, rides 133\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 781, reward 1015.0, memory_length 2000, epsilon 0.04370622618159837, time 728.0, rides 126\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 782, reward 652.0, memory_length 2000, epsilon 0.043531401276871974, time 721.0, rides 147\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 783, reward 692.0, memory_length 2000, epsilon 0.043357275671764485, time 722.0, rides 132\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 784, reward 1140.0, memory_length 2000, epsilon 0.04318384656907743, time 724.0, rides 111\n",
      "Initial State is  [2, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 785, reward 1048.0, memory_length 2000, epsilon 0.04301111118280112, time 726.0, rides 114\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 786, reward 1014.0, memory_length 2000, epsilon 0.04283906673806991, time 729.0, rides 123\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 787, reward 1079.0, memory_length 2000, epsilon 0.042667710471117636, time 724.0, rides 124\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 788, reward 889.0, memory_length 2000, epsilon 0.042497039629233166, time 732.0, rides 109\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 789, reward 1271.0, memory_length 2000, epsilon 0.042327051470716234, time 723.0, rides 117\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 790, reward 955.0, memory_length 2000, epsilon 0.04215774326483337, time 726.0, rides 119\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 791, reward 1005.0, memory_length 2000, epsilon 0.04198911229177403, time 727.0, rides 125\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 792, reward 895.0, memory_length 2000, epsilon 0.041821155842606934, time 721.0, rides 118\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 793, reward 862.0, memory_length 2000, epsilon 0.041653871219236506, time 729.0, rides 120\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 794, reward 926.0, memory_length 2000, epsilon 0.04148725573435956, time 730.0, rides 132\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 795, reward 1073.0, memory_length 2000, epsilon 0.041321306711422125, time 730.0, rides 144\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 796, reward 976.0, memory_length 2000, epsilon 0.041156021484576435, time 727.0, rides 122\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 797, reward 1180.0, memory_length 2000, epsilon 0.04099139739863813, time 732.0, rides 130\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 798, reward 832.0, memory_length 2000, epsilon 0.04082743180904358, time 723.0, rides 112\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 799, reward 904.0, memory_length 2000, epsilon 0.040664122081807405, time 728.0, rides 117\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 800, reward 777.0, memory_length 2000, epsilon 0.040501465593480175, time 726.0, rides 116\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 801, reward 903.0, memory_length 2000, epsilon 0.04033945973110625, time 733.0, rides 128\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 802, reward 922.0, memory_length 2000, epsilon 0.04017810189218183, time 733.0, rides 122\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 803, reward 774.0, memory_length 2000, epsilon 0.0400173894846131, time 723.0, rides 104\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 804, reward 746.0, memory_length 2000, epsilon 0.03985731992667465, time 734.0, rides 128\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 805, reward 675.0, memory_length 2000, epsilon 0.03969789064696795, time 727.0, rides 111\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 806, reward 1252.0, memory_length 2000, epsilon 0.039539099084380074, time 730.0, rides 117\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 807, reward 945.0, memory_length 2000, epsilon 0.039380942688042556, time 731.0, rides 120\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 808, reward 1149.0, memory_length 2000, epsilon 0.039223418917290385, time 722.0, rides 125\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 809, reward 867.0, memory_length 2000, epsilon 0.03906652524162122, time 726.0, rides 123\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 810, reward 1047.0, memory_length 2000, epsilon 0.03891025914065474, time 733.0, rides 118\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 811, reward 1129.0, memory_length 2000, epsilon 0.03875461810409212, time 729.0, rides 125\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 812, reward 955.0, memory_length 2000, epsilon 0.03859959963167575, time 730.0, rides 114\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 813, reward 940.0, memory_length 2000, epsilon 0.03844520123314905, time 731.0, rides 125\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 814, reward 953.0, memory_length 2000, epsilon 0.038291420428216455, time 724.0, rides 127\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 815, reward 1123.0, memory_length 2000, epsilon 0.038138254746503585, time 726.0, rides 122\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 816, reward 775.0, memory_length 2000, epsilon 0.03798570172751757, time 727.0, rides 126\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 817, reward 710.0, memory_length 2000, epsilon 0.0378337589206075, time 723.0, rides 121\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 818, reward 970.0, memory_length 2000, epsilon 0.03768242388492507, time 726.0, rides 130\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 819, reward 879.0, memory_length 2000, epsilon 0.03753169418938537, time 720.0, rides 120\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 820, reward 1072.0, memory_length 2000, epsilon 0.037381567412627825, time 734.0, rides 117\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 821, reward 913.0, memory_length 2000, epsilon 0.03723204114297732, time 731.0, rides 126\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 822, reward 664.0, memory_length 2000, epsilon 0.03708311297840541, time 731.0, rides 116\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 823, reward 1006.0, memory_length 2000, epsilon 0.03693478052649179, time 732.0, rides 128\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 824, reward 793.0, memory_length 2000, epsilon 0.036787041404385816, time 734.0, rides 118\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 825, reward 821.0, memory_length 2000, epsilon 0.036639893238768276, time 735.0, rides 138\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 826, reward 998.0, memory_length 2000, epsilon 0.0364933336658132, time 722.0, rides 117\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 827, reward 871.0, memory_length 2000, epsilon 0.03634736033114995, time 733.0, rides 126\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 828, reward 753.0, memory_length 2000, epsilon 0.03620197088982535, time 727.0, rides 123\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 829, reward 1132.0, memory_length 2000, epsilon 0.036057163006266046, time 728.0, rides 133\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 830, reward 1033.0, memory_length 2000, epsilon 0.035912934354240984, time 726.0, rides 125\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 831, reward 995.0, memory_length 2000, epsilon 0.03576928261682402, time 734.0, rides 129\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 832, reward 807.0, memory_length 2000, epsilon 0.03562620548635672, time 731.0, rides 133\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 833, reward 781.0, memory_length 2000, epsilon 0.0354837006644113, time 729.0, rides 125\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 834, reward 805.0, memory_length 2000, epsilon 0.03534176586175365, time 723.0, rides 123\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 835, reward 690.0, memory_length 2000, epsilon 0.03520039879830664, time 739.0, rides 133\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 836, reward 1244.0, memory_length 2000, epsilon 0.03505959720311341, time 736.0, rides 123\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 837, reward 868.0, memory_length 2000, epsilon 0.03491935881430096, time 731.0, rides 134\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 838, reward 985.0, memory_length 2000, epsilon 0.034779681379043755, time 733.0, rides 133\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 839, reward 826.0, memory_length 2000, epsilon 0.03464056265352758, time 725.0, rides 125\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 840, reward 1076.0, memory_length 2000, epsilon 0.034502000402913464, time 721.0, rides 131\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 841, reward 1085.0, memory_length 2000, epsilon 0.034363992401301814, time 733.0, rides 121\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 842, reward 898.0, memory_length 2000, epsilon 0.034226536431696604, time 731.0, rides 119\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 843, reward 1129.0, memory_length 2000, epsilon 0.03408963028596982, time 726.0, rides 118\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 844, reward 814.0, memory_length 2000, epsilon 0.03395327176482594, time 724.0, rides 125\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 845, reward 1082.0, memory_length 2000, epsilon 0.033817458677766636, time 742.0, rides 128\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 846, reward 810.0, memory_length 2000, epsilon 0.03368218884305557, time 736.0, rides 120\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 847, reward 913.0, memory_length 2000, epsilon 0.03354746008768335, time 728.0, rides 121\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 848, reward 773.0, memory_length 2000, epsilon 0.033413270247332615, time 728.0, rides 119\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 849, reward 1105.0, memory_length 2000, epsilon 0.033279617166343284, time 727.0, rides 125\n",
      "Initial State is  [1, 15, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 850, reward 898.0, memory_length 2000, epsilon 0.03314649869767791, time 732.0, rides 118\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 851, reward 1065.0, memory_length 2000, epsilon 0.0330139127028872, time 731.0, rides 119\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 852, reward 1286.0, memory_length 2000, epsilon 0.03288185705207565, time 730.0, rides 124\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 853, reward 1269.0, memory_length 2000, epsilon 0.03275032962386735, time 734.0, rides 139\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 854, reward 1190.0, memory_length 2000, epsilon 0.03261932830537188, time 724.0, rides 131\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 855, reward 973.0, memory_length 2000, epsilon 0.03248885099215039, time 720.0, rides 134\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 856, reward 970.0, memory_length 2000, epsilon 0.03235889558818179, time 723.0, rides 135\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 857, reward 1488.0, memory_length 2000, epsilon 0.03222946000582906, time 728.0, rides 126\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 858, reward 1215.0, memory_length 2000, epsilon 0.03210054216580574, time 731.0, rides 120\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 859, reward 839.0, memory_length 2000, epsilon 0.03197213999714252, time 733.0, rides 123\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 860, reward 931.0, memory_length 2000, epsilon 0.03184425143715395, time 732.0, rides 111\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 861, reward 769.0, memory_length 2000, epsilon 0.031716874431405334, time 729.0, rides 113\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 862, reward 1035.0, memory_length 2000, epsilon 0.031590006933679714, time 731.0, rides 115\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 863, reward 1287.0, memory_length 2000, epsilon 0.03146364690594499, time 733.0, rides 120\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 864, reward 998.0, memory_length 2000, epsilon 0.03133779231832121, time 727.0, rides 133\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 865, reward 1248.0, memory_length 2000, epsilon 0.031212441149047927, time 731.0, rides 122\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 866, reward 975.0, memory_length 2000, epsilon 0.031087591384451736, time 727.0, rides 135\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 867, reward 1043.0, memory_length 2000, epsilon 0.03096324101891393, time 726.0, rides 141\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 868, reward 1028.0, memory_length 2000, epsilon 0.030839388054838275, time 726.0, rides 128\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 869, reward 1040.0, memory_length 2000, epsilon 0.03071603050261892, time 730.0, rides 120\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 870, reward 859.0, memory_length 2000, epsilon 0.030593166380608446, time 727.0, rides 107\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 871, reward 1097.0, memory_length 2000, epsilon 0.03047079371508601, time 729.0, rides 113\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 872, reward 765.0, memory_length 2000, epsilon 0.030348910540225666, time 731.0, rides 137\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 873, reward 1130.0, memory_length 2000, epsilon 0.030227514898064765, time 730.0, rides 134\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 874, reward 965.0, memory_length 2000, epsilon 0.030106604838472505, time 723.0, rides 129\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 875, reward 919.0, memory_length 2000, epsilon 0.029986178419118614, time 736.0, rides 123\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 876, reward 970.0, memory_length 2000, epsilon 0.02986623370544214, time 731.0, rides 139\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 877, reward 977.0, memory_length 2000, epsilon 0.02974676877062037, time 729.0, rides 133\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 878, reward 1019.0, memory_length 2000, epsilon 0.02962778169553789, time 724.0, rides 132\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 879, reward 921.0, memory_length 2000, epsilon 0.029509270568755738, time 723.0, rides 130\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 880, reward 1036.0, memory_length 2000, epsilon 0.029391233486480716, time 736.0, rides 123\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 881, reward 841.0, memory_length 2000, epsilon 0.02927366855253479, time 731.0, rides 127\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 882, reward 891.0, memory_length 2000, epsilon 0.029156573878324654, time 732.0, rides 125\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 883, reward 1140.0, memory_length 2000, epsilon 0.029039947582811355, time 727.0, rides 137\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 884, reward 1070.0, memory_length 2000, epsilon 0.02892378779248011, time 731.0, rides 119\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 885, reward 863.0, memory_length 2000, epsilon 0.02880809264131019, time 731.0, rides 127\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 886, reward 793.0, memory_length 2000, epsilon 0.028692860270744948, time 730.0, rides 127\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 887, reward 1195.0, memory_length 2000, epsilon 0.028578088829661966, time 726.0, rides 126\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 888, reward 824.0, memory_length 2000, epsilon 0.028463776474343318, time 721.0, rides 119\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 889, reward 1127.0, memory_length 2000, epsilon 0.028349921368445944, time 730.0, rides 129\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 890, reward 1277.0, memory_length 2000, epsilon 0.028236521682972162, time 730.0, rides 119\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 891, reward 1268.0, memory_length 2000, epsilon 0.028123575596240274, time 738.0, rides 132\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 892, reward 735.0, memory_length 2000, epsilon 0.028011081293855312, time 727.0, rides 132\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 893, reward 719.0, memory_length 2000, epsilon 0.027899036968679892, time 734.0, rides 127\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 894, reward 1043.0, memory_length 2000, epsilon 0.02778744082080517, time 728.0, rides 134\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 895, reward 1150.0, memory_length 2000, epsilon 0.02767629105752195, time 730.0, rides 141\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 896, reward 1165.0, memory_length 2000, epsilon 0.02756558589329186, time 727.0, rides 127\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 897, reward 1117.0, memory_length 2000, epsilon 0.027455323549718694, time 729.0, rides 135\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 898, reward 1146.0, memory_length 2000, epsilon 0.02734550225551982, time 726.0, rides 128\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 899, reward 806.0, memory_length 2000, epsilon 0.02723612024649774, time 732.0, rides 130\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 900, reward 753.0, memory_length 2000, epsilon 0.02712717576551175, time 732.0, rides 118\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 901, reward 1093.0, memory_length 2000, epsilon 0.0270186670624497, time 733.0, rides 128\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 902, reward 1161.0, memory_length 2000, epsilon 0.026910592394199902, time 729.0, rides 130\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 903, reward 819.0, memory_length 2000, epsilon 0.026802950024623105, time 728.0, rides 133\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 904, reward 840.0, memory_length 2000, epsilon 0.02669573822452461, time 734.0, rides 115\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 905, reward 1181.0, memory_length 2000, epsilon 0.026588955271626514, time 735.0, rides 128\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 906, reward 1111.0, memory_length 2000, epsilon 0.026482599450540007, time 736.0, rides 124\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 907, reward 1149.0, memory_length 2000, epsilon 0.026376669052737847, time 731.0, rides 124\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 908, reward 978.0, memory_length 2000, epsilon 0.026271162376526894, time 734.0, rides 121\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 909, reward 880.0, memory_length 2000, epsilon 0.026166077727020787, time 730.0, rides 136\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 910, reward 1124.0, memory_length 2000, epsilon 0.026061413416112705, time 725.0, rides 133\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 911, reward 813.0, memory_length 2000, epsilon 0.025957167762448254, time 726.0, rides 121\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 912, reward 966.0, memory_length 2000, epsilon 0.02585333909139846, time 723.0, rides 119\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 913, reward 1112.0, memory_length 2000, epsilon 0.025749925735032864, time 740.0, rides 121\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 914, reward 985.0, memory_length 2000, epsilon 0.025646926032092735, time 728.0, rides 121\n",
      "Initial State is  [3, 0, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 915, reward 987.0, memory_length 2000, epsilon 0.025544338327964364, time 723.0, rides 124\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 916, reward 922.0, memory_length 2000, epsilon 0.025442160974652506, time 734.0, rides 116\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 917, reward 1084.0, memory_length 2000, epsilon 0.025340392330753896, time 738.0, rides 123\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 918, reward 1016.0, memory_length 2000, epsilon 0.02523903076143088, time 726.0, rides 131\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 919, reward 843.0, memory_length 2000, epsilon 0.025138074638385157, time 734.0, rides 129\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 920, reward 1176.0, memory_length 2000, epsilon 0.025037522339831617, time 736.0, rides 129\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 921, reward 1204.0, memory_length 2000, epsilon 0.024937372250472288, time 726.0, rides 136\n",
      "Initial State is  [1, 7, 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4ba5b7bf136a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# 4. Train the model by calling function agent.train_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mloop\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-84b63840990d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# 2. Get the target for the Q-network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mtarget_qval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch_1.7.1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch_1.7.1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch_1.7.1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch_1.7.1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch_1.7.1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 705\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch_1.7.1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2968\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2970\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2971\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   2972\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(36,21)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env= CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "    state_space = env.state_space\n",
    "    action_space = env.action_space\n",
    "    \n",
    "    state = env.state_init\n",
    "    print(\"Initial State is \",state)\n",
    "    time= 0\n",
    "\n",
    "    #Call the DQN agent\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    \n",
    "    action = agent.get_action(env.state_encod_arch1(state), env)\n",
    "    score += env.reward_func(state,action_space[action], Time_matrix)\n",
    "    next_state,ride_time = env.next_state_func(state,action_space[action], Time_matrix)\n",
    "    time += ride_time\n",
    "    \n",
    "    if time >= 24*30:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),True)\n",
    "    else:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),False)\n",
    "    \n",
    "    loop = 0\n",
    "    \n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        if time >= 24*30:\n",
    "            terminal_state= True\n",
    "            pass\n",
    "        \n",
    "        state= next_state\n",
    "        \n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "        \n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward_curr_ride = env.reward_func(state,action_space[action],Time_matrix)\n",
    "        score+= reward_curr_ride\n",
    "        next_state,ride_time = env.next_state_func(next_state,action_space[action],Time_matrix)\n",
    "        time += ride_time\n",
    "        \n",
    "        # 3. Append the experience to the memory\n",
    "        if time >= 24*30:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),True)\n",
    "        else:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),False)\n",
    "        \n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model(env)\n",
    "        loop+=1\n",
    "        \n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "    rewards_per_episode.append(score)   \n",
    "    episodes.append(episode)\n",
    "    \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon = agent.epsilon_max*((1-agent.epsilon_decay)**episode)\n",
    "        \n",
    "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, time {4}, rides {5}\".format(episode,\n",
    "                                                                                                score,\n",
    "                                                                                                len(agent.memory),\n",
    "                                                                                                agent.epsilon,\n",
    "                                                                                                time,\n",
    "                                                                                                loop))\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        agent.save(name= \"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-nebraska",
   "metadata": {},
   "source": [
    "## Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "short-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a weighted average function\n",
    "def wt_average(arr, period= 10):\n",
    "    wt_arr= []\n",
    "    arr_sum= 0\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        arr_sum+= arr[i]\n",
    "        if i%period == 0:\n",
    "            wt_arr.append(arr_sum/period)\n",
    "            arr_sum= 0\n",
    "    return wt_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spiritual-leader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bf4cd5a8b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting convergence with the 2 step moving average\n",
    "plt.figure(figsize= (10, 6))\n",
    "plt.plot(rewards_per_episode, label= 'rewards per episode')\n",
    "plt.plot(wt_average(rewards_per_episode, 2), label= '2 step moving average')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-fever",
   "metadata": {},
   "source": [
    "## Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "senior-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining epsilon decay function\n",
    "def epsilon_decay(ep_count,epsilon_decay= 4e-3):\n",
    "    epsilon = 1*((1- epsilon_decay)**ep_count) \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "electrical-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating epsilon values\n",
    "epsilon= [epsilon_decay(x) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "published-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bd6ac58640>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting epsilon values\n",
    "plt.plot(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-constitution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
