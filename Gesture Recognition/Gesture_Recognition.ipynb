{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from cv2 import resize\n",
    "from imageio import imread\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import debugging\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import TimeDistributed\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity('ERROR')\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom generator is a class MyGen which extends the keras.utils.Sequence and overrides all the necessary methods\n",
    "# This method was chosen over a simple generator as it provides better modularity as easier modification of methods.\n",
    "\n",
    "class MyGen(keras.utils.Sequence):\n",
    "    # The initializer for the MyGen class\n",
    "    def __init__(self, source_path, batch_size, n_images= 20, im_size= 200):\n",
    "        self.l = []\n",
    "        self.batch_size = batch_size  # batch_size, i.e. no. of videos in a batch\n",
    "        self.n_images= n_images       # no. of frames per video\n",
    "        self.im_size= im_size         # hxw of the image to resize,'h'='w' as we have opted to resize the images to a square\n",
    "        self.source_path = source_path # source path of the file (videos)\n",
    "        self.folder_names = np.array(os.listdir(self.source_path))  # ndarray of 'folder_names'\n",
    "        np.random.shuffle(self.folder_names)   # shuffling the 'folder_names'\n",
    "        \n",
    "        # A dictionary assigning values arbitarily to the gestures\n",
    "        self.gestures= {'Swipe Right': 0, 'Swipe Left': 1, 'Stop': 2, 'Thumbs Down': 3, 'Thumbs Up': 4}\n",
    "        \n",
    "        # Creating a one_hot_vec for each gesture\n",
    "        # this is a 5x5 ndarray each column represents a vector for the corresponding gesture\n",
    "        self.one_hot_vec= np.zeros_like(0., shape= (5,5))\n",
    "        for rw in range(0, 5):\n",
    "            for cl in range(0, 5):\n",
    "                if rw==cl:\n",
    "                    self.one_hot_vec[rw, cl]+= 1.\n",
    "                    break\n",
    "        \n",
    "        # Extracting the labels for the folders depending on their names using a custom method 'FLabels()'\n",
    "        self.folder_labels= []\n",
    "        for L in self.folder_names:\n",
    "            self.folder_labels.append((self.one_hot_vec[self.gestures[self.FLabels(L)]]))\n",
    "            \n",
    "    # This method represents the no. of batches of a given size in the dataset\n",
    "    def __len__(self):  \n",
    "        return math.floor(len(self.folder_names)/ self.batch_size)\n",
    "    \n",
    "    # This method fetches the batches\n",
    "    def __getitem__(self, idx): \n",
    "        self.l.append(idx)\n",
    "        self.batch_count= 0\n",
    "        self.batch_names= self.folder_names[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "        self.batch_labels= self.folder_labels[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "        \n",
    "        self.batch_data= np.zeros_like(0., shape= (self.batch_size, self.n_images, self.im_size, self.im_size, 3))\n",
    "        \n",
    "        for name in self.batch_names:\n",
    "            self.frm= os.listdir(f'{self.source_path}{name}/')\n",
    "            if len(self.frm) > self.n_images:\n",
    "                self.diff= len(self.frm) - self.n_images\n",
    "                self.frm= self.frm[math.floor(self.diff/2):(len(self.frm) - math.ceil(self.diff/2))]\n",
    "            self.frm_add= [ (f'{name}/' + f) for f in self.frm ] \n",
    "            \n",
    "            # The frames are resized and normalized by dividing each pixel by 255. \n",
    "            self.frames= np.array([resize(imread(f'{self.source_path}{img}'), (self.im_size,self.im_size))/255. for img in self.frm_add])\n",
    "        \n",
    "            self.batch_data[self.batch_count]= self.frames\n",
    "            self.batch_count+=1\n",
    "        \n",
    "        self.batch_x = self.batch_data.astype(np.float32)\n",
    "        self.batch_y = np.array(self.batch_labels)\n",
    "        \n",
    "        return self.batch_x, self.batch_y\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Method to extract the labels of the videos\n",
    "    def FLabels(self, arr):\n",
    "        self.arr = arr\n",
    "        if ('Thumbs' in self.arr) or ('thumbs' in self.arr):\n",
    "            if ('Down' in self.arr) or ('down' in self.arr):\n",
    "                return 'Thumbs Down'\n",
    "            else:\n",
    "                return 'Thumbs Up'\n",
    "    \n",
    "        elif ('Swipe' in self.arr) or ('swipe' in self.arr):\n",
    "            if ('Left' in self.arr) or ('left' in self.arr):\n",
    "                return 'Swipe Left'\n",
    "            else:\n",
    "                return 'Swipe Right'\n",
    "        elif ('Stop' in self.arr) or ('stop' in self.arr):\n",
    "            return 'Stop'\n",
    "        else:\n",
    "            return np.NaN      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "# Latest Date time, this will be used for the checkpoints\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "time= str(curr_dt_time.time())\n",
    "date= str(curr_dt_time.date())\n",
    "T= time.split(':')\n",
    "new_time= '_' + T[0]+ '_' + T[1]\n",
    "new_date= date.replace('-', '_')\n",
    "new_date_time= new_date + new_time\n",
    "\n",
    "\n",
    "train_path = './Project_data/train/'\n",
    "val_path = './Project_data/val/'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.Sequential([\n",
    "    # Input layer for the model\n",
    "    # '25' is the no. of images/frames of the video selected for analysis\n",
    "    # '120' is the shape of the image, i.e. it is both the height and the width of the image\n",
    "    # '3' is the no. of channels RGB\n",
    "    keras.layers.InputLayer(input_shape= (25,120,120,3)),\n",
    "    \n",
    "    # TimeDistributed 2D Convolutions to convolve each frame respectively\n",
    "    # Note: Channels here would be refering to the no. of 3D solids outputted by the convolutions keeping, \n",
    "    #       the 'RGB' channels constant, essentially we are increasing the no. of frames per video in these\n",
    "    #       layers this convention will be followed in later models also\n",
    "    \n",
    "    # In this block the following are the layers:\n",
    "    # 1.'TimeDistributed 3x3 Conv2D',outputting 25 channels, activation 'relu'\n",
    "    # 2.'TimeDistributed 2x2 MaxPool2D'\n",
    "    # 3.'TimeDistributed Dropout layer', rate '0.2'\n",
    "    # This is repeated a second time on with the Conv2D layer outputting 35 channels\n",
    "    # and the Dropout layer has a rate of '0.3'\n",
    "    # This marks the end of the TimeDistributed 2D Convolutions\n",
    "    \n",
    "    TimeDistributed(keras.layers.Conv2D(25, 3, activation= 'relu')),\n",
    "    TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), padding='same')),\n",
    "    TimeDistributed(keras.layers.Dropout(rate= 0.2)),\n",
    "    TimeDistributed(keras.layers.Conv2D(35, 3, activation= 'relu')),\n",
    "    TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), padding= 'same')),\n",
    "    TimeDistributed(keras.layers.Dropout(rate= 0.3)),\n",
    "    \n",
    "    #3D Convolutions to convolve over the time axis\n",
    "    # 3D Convolutions to convolve over the time axis\n",
    "    # Note: Channels here would be refering to the no. of 3D solids outputted by the convolutions, this convention will\n",
    "    #       be followed in later models also.\n",
    "    \n",
    "    # Following are the layers:\n",
    "    # 1. '3x3x3 Conv3D', outputting 10 channels with an activation of 'relu'\n",
    "    # 2. '2x2x2 MaxPool3D'\n",
    "    # 3. '1x2x2 MaxPool3D', This is aimed at preserving the data on the time axis, as 3D convolutions reduce data on all axes\n",
    "    # This marks the end of the 3D convolutions\n",
    "    keras.layers.Conv3D(10, 3, activation= 'relu'),\n",
    "    keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding= 'same'),\n",
    "    keras.layers.MaxPool3D(pool_size=(1, 2, 2), padding= 'same'),\n",
    "    \n",
    "    # TimeDistributed Flattened layer\n",
    "    TimeDistributed(keras.layers.Flatten()),\n",
    "    \n",
    "    # RNN Layers\n",
    "    \n",
    "    # GRU Layer with return_sequences= 'True' outputs the data for every timestep, ie it preserves the 'TimeDistributed'\n",
    "    # property of the data\n",
    "    \n",
    "    # Following are the specifications for the GRU Layer:\n",
    "    # output_dim: 10; Reason: Experimentally determine optimal value\n",
    "    # return_sequences: True; Reason: Explained above\n",
    "    # activation: 'relu'; Reason: Found to provide optimal results for image data\n",
    "    # kernel_regularizer= 'l2'; Reason: Experimentally determine optimal value\n",
    "    # recurrent_regularizer= 'l1'; Reason: Experimentally determine optimal value\n",
    "    # use_bias= True; Reason: Experimentally determine optimal value\n",
    "    # bias_initializer= 'zeros'; Reason: initializing bias with zeros helps the model to learn biases from scratch\n",
    "    \n",
    "    keras.layers.GRU(10, \n",
    "                     return_sequences= True, \n",
    "                     activation= 'relu', \n",
    "                     kernel_regularizer= 'l2',\n",
    "                     recurrent_regularizer= 'l1',\n",
    "                     use_bias= True,\n",
    "                     bias_initializer= 'zeros',\n",
    "                     ),\n",
    "    \n",
    "    # GRU Layer with return_sequences= 'False' outputs the data for the final timestep,ie it would be like \n",
    "    # watching the data in a sequence, and predicting the outcome\n",
    "    \n",
    "    # Following are the specifications for the GRU Layer:\n",
    "    # output_dim: 5; Reason: Only 5 classes present\n",
    "    # return_sequences: False; Reason: Explained above\n",
    "    # activation: 'softmax'; Reason: Output layer must be softmax for crossentropy loss\n",
    "    # kernel_regularizer= 'l2'; Reason: Experimentally determine optimal value\n",
    "    # recurrent_regularizer= 'l1'; Reason: Experimentally determine optimal value\n",
    "    # use_bias= True; Reason: Experimentally determine optimal value\n",
    "    # bias_initializer= 'zeros'; Reason: initializing bias with zeros helps the model to learn biases from scratch\n",
    "    \n",
    "    keras.layers.GRU(5, \n",
    "                     return_sequences= False, \n",
    "                     activation= 'softmax', \n",
    "                     kernel_regularizer= 'l2',\n",
    "                     recurrent_regularizer= 'l1',\n",
    "                     use_bias= True,\n",
    "                     bias_initializer= 'zeros',\n",
    "                     )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_14 (TimeDis (None, 25, 118, 118, 25)  700       \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 25, 59, 59, 25)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 25, 59, 59, 25)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 25, 57, 57, 35)    7910      \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 25, 29, 29, 35)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 25, 29, 29, 35)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 23, 27, 27, 10)    9460      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 12, 14, 14, 10)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 12, 7, 7, 10)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 12, 490)           0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 12, 10)            15060     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 33,385\n",
      "Trainable params: 33,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model with:\n",
    "# optimizer: 'adam'\n",
    "# loss: 'categorical_crossentropy'\n",
    "# metrics: 'categorical_accuracy'\n",
    "# run_eagerly: True\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics=['categorical_accuracy'], run_eagerly= True)\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a batch size 20\n",
    "batch_size= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the train and val generators with:\n",
    "# batch_size: 20\n",
    "# im_size: 120\n",
    "# n_images: 25\n",
    "train_generator = MyGen(train_path, batch_size, im_size=120, n_images= 25)\n",
    "val_generator = MyGen(val_path, batch_size, im_size=120, n_images= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model name\n",
    "model_name = 'model_1' + '_' + new_date_time + '.h5'\n",
    "    \n",
    "# Creating a path to store model\n",
    "if not os.path.exists(f'./models/{model_name}/'):\n",
    "    os.mkdir(f'./models/{model_name}/')\n",
    "    filepath = f'./models/{model_name}/'    \n",
    "else:\n",
    "    os.mkdir(f'./models/{model_name}_2/')\n",
    "    filepath = f'./models/{model_name}_2/'\n",
    "        \n",
    "# Setting up the model checkpoint and the callbacks_list\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=2, save_best_only=True, save_weights_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the steps per epoch for train and val generators\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int((num_train_sequences//batch_size) - 1)\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = int((num_val_sequences//batch_size) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Number of epochs to 35\n",
    "num_epochs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 2.6578 - categorical_accuracy: 0.2000\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.23000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 2.6578 - categorical_accuracy: 0.2000 - val_loss: 2.4362 - val_categorical_accuracy: 0.2300\n",
      "Epoch 2/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 2.2916 - categorical_accuracy: 0.2172\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.23000 to 0.26000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 35s 1s/step - loss: 2.2916 - categorical_accuracy: 0.2172 - val_loss: 2.1521 - val_categorical_accuracy: 0.2600\n",
      "Epoch 3/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 2.0328 - categorical_accuracy: 0.2672\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.26000 to 0.41000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 2.0328 - categorical_accuracy: 0.2672 - val_loss: 1.9244 - val_categorical_accuracy: 0.4100\n",
      "Epoch 4/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.8200 - categorical_accuracy: 0.3406\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.41000 to 0.43000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 35s 1s/step - loss: 1.8200 - categorical_accuracy: 0.3406 - val_loss: 1.6954 - val_categorical_accuracy: 0.4300\n",
      "Epoch 5/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.6441 - categorical_accuracy: 0.3906\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.43000 to 0.46000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 1.6441 - categorical_accuracy: 0.3906 - val_loss: 1.5104 - val_categorical_accuracy: 0.4600\n",
      "Epoch 6/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.4638 - categorical_accuracy: 0.4609\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.46000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 1.4638 - categorical_accuracy: 0.4609 - val_loss: 1.3767 - val_categorical_accuracy: 0.4600\n",
      "Epoch 7/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3065 - categorical_accuracy: 0.5094\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.46000 to 0.53000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 1.3065 - categorical_accuracy: 0.5094 - val_loss: 1.2841 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.1884 - categorical_accuracy: 0.5500\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.53000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 1.1884 - categorical_accuracy: 0.5500 - val_loss: 1.3260 - val_categorical_accuracy: 0.5200\n",
      "Epoch 9/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.0786 - categorical_accuracy: 0.6109\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.53000 to 0.60000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 1.0786 - categorical_accuracy: 0.6109 - val_loss: 1.0657 - val_categorical_accuracy: 0.6000\n",
      "Epoch 10/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.0045 - categorical_accuracy: 0.6328\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.60000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 1.0045 - categorical_accuracy: 0.6328 - val_loss: 1.0623 - val_categorical_accuracy: 0.5700\n",
      "Epoch 11/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8462 - categorical_accuracy: 0.7063\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.60000 to 0.62000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.8462 - categorical_accuracy: 0.7063 - val_loss: 1.0536 - val_categorical_accuracy: 0.6200\n",
      "Epoch 12/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8546 - categorical_accuracy: 0.6953\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.62000 to 0.63000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.8546 - categorical_accuracy: 0.6953 - val_loss: 0.9008 - val_categorical_accuracy: 0.6300\n",
      "Epoch 13/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7105 - categorical_accuracy: 0.7547\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.63000 to 0.66000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.7105 - categorical_accuracy: 0.7547 - val_loss: 0.8779 - val_categorical_accuracy: 0.6600\n",
      "Epoch 14/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6563 - categorical_accuracy: 0.7781\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.66000 to 0.68000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.6563 - categorical_accuracy: 0.7781 - val_loss: 0.8585 - val_categorical_accuracy: 0.6800\n",
      "Epoch 15/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6265 - categorical_accuracy: 0.7891\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.68000 to 0.72000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.6265 - categorical_accuracy: 0.7891 - val_loss: 0.8406 - val_categorical_accuracy: 0.7200\n",
      "Epoch 16/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5791 - categorical_accuracy: 0.8047\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.72000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.5791 - categorical_accuracy: 0.8047 - val_loss: 0.8576 - val_categorical_accuracy: 0.7100\n",
      "Epoch 17/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5934 - categorical_accuracy: 0.7953\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.72000\n",
      "32/32 [==============================] - 35s 1s/step - loss: 0.5934 - categorical_accuracy: 0.7953 - val_loss: 0.8584 - val_categorical_accuracy: 0.6600\n",
      "Epoch 18/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5323 - categorical_accuracy: 0.8266\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.72000\n",
      "32/32 [==============================] - 35s 1s/step - loss: 0.5323 - categorical_accuracy: 0.8266 - val_loss: 0.8196 - val_categorical_accuracy: 0.7100\n",
      "Epoch 19/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5511 - categorical_accuracy: 0.8125\n",
      "Epoch 00019: val_categorical_accuracy improved from 0.72000 to 0.73000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.5511 - categorical_accuracy: 0.8125 - val_loss: 0.8308 - val_categorical_accuracy: 0.7300\n",
      "Epoch 20/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4649 - categorical_accuracy: 0.8516\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.73000 to 0.75000, saving model to ./models\\model_1_2021_01_04_22_43.h5\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.4649 - categorical_accuracy: 0.8516 - val_loss: 0.8018 - val_categorical_accuracy: 0.7500\n",
      "Epoch 21/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4379 - categorical_accuracy: 0.8719\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.4379 - categorical_accuracy: 0.8719 - val_loss: 0.9299 - val_categorical_accuracy: 0.7000\n",
      "Epoch 22/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4310 - categorical_accuracy: 0.8719\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.4310 - categorical_accuracy: 0.8719 - val_loss: 0.9480 - val_categorical_accuracy: 0.7400\n",
      "Epoch 23/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4542 - categorical_accuracy: 0.8516\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.4542 - categorical_accuracy: 0.8516 - val_loss: 0.8220 - val_categorical_accuracy: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3775 - categorical_accuracy: 0.8828\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.3775 - categorical_accuracy: 0.8828 - val_loss: 0.8582 - val_categorical_accuracy: 0.7400\n",
      "Epoch 25/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3325 - categorical_accuracy: 0.9047\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.3325 - categorical_accuracy: 0.9047 - val_loss: 0.9193 - val_categorical_accuracy: 0.7200\n",
      "Epoch 26/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3152 - categorical_accuracy: 0.9125\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 35s 1s/step - loss: 0.3152 - categorical_accuracy: 0.9125 - val_loss: 1.0370 - val_categorical_accuracy: 0.7100\n",
      "Epoch 27/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3082 - categorical_accuracy: 0.9109\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.3082 - categorical_accuracy: 0.9109 - val_loss: 1.0671 - val_categorical_accuracy: 0.7200\n",
      "Epoch 28/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3445 - categorical_accuracy: 0.9031\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.3445 - categorical_accuracy: 0.9031 - val_loss: 0.9612 - val_categorical_accuracy: 0.7500\n",
      "Epoch 29/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2815 - categorical_accuracy: 0.9266\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.2815 - categorical_accuracy: 0.9266 - val_loss: 1.0334 - val_categorical_accuracy: 0.7300\n",
      "Epoch 30/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2387 - categorical_accuracy: 0.9438\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.2387 - categorical_accuracy: 0.9438 - val_loss: 1.2099 - val_categorical_accuracy: 0.7100\n",
      "Epoch 31/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2204 - categorical_accuracy: 0.9563\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.2204 - categorical_accuracy: 0.9563 - val_loss: 1.1453 - val_categorical_accuracy: 0.7000\n",
      "Epoch 32/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2024 - categorical_accuracy: 0.9703\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.2024 - categorical_accuracy: 0.9703 - val_loss: 1.1650 - val_categorical_accuracy: 0.6800\n",
      "Epoch 33/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1840 - categorical_accuracy: 0.9672\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.1840 - categorical_accuracy: 0.9672 - val_loss: 1.1299 - val_categorical_accuracy: 0.7200\n",
      "Epoch 34/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2992 - categorical_accuracy: 0.9297\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 35s 1s/step - loss: 0.2992 - categorical_accuracy: 0.9297 - val_loss: 0.9922 - val_categorical_accuracy: 0.7300\n",
      "Epoch 35/35\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1963 - categorical_accuracy: 0.9766\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.75000\n",
      "32/32 [==============================] - 36s 1s/step - loss: 0.1963 - categorical_accuracy: 0.9766 - val_loss: 1.1040 - val_categorical_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Fitting the first model\n",
    "history= model.fit(\n",
    "    train_generator, \n",
    "    epochs= num_epochs,\n",
    "    steps_per_epoch= steps_per_epoch,\n",
    "    verbose= 1, \n",
    "    callbacks= callbacks_list, \n",
    "    validation_data= val_generator,\n",
    "    validation_steps= validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy and loss\n",
    "vacc_1= history.history.get('val_categorical_accuracy')\n",
    "vloss_1= history.history.get('val_loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy and loss\n",
    "cacc_1= history.history.get('categorical_accuracy')\n",
    "closs_1= history.history.get('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "# Latest Date time, this will be used for the checkpoints\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "time= str(curr_dt_time.time())\n",
    "date= str(curr_dt_time.date())\n",
    "T= time.split(':')\n",
    "new_time= '_' + T[0]+ '_' + T[1]\n",
    "new_date= date.replace('-', '_')\n",
    "new_date_time= new_date + new_time\n",
    "\n",
    "\n",
    "train_path = './Project_data/train/'\n",
    "val_path = './Project_data/val/'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.Sequential([\n",
    "    # Input layer for the model\n",
    "    # '20' is the no. of images/frames of the video selected for analysis we have reduced it from '25'\n",
    "    # '200' is the shape of the image, i.e. it is both the height and the width of the image\n",
    "    # '3' is the no. of channels RGB\n",
    "    keras.layers.InputLayer(input_shape= (25,200,200,3)),\n",
    "    \n",
    "    # TimeDistributed 2D Convolutions to convolve each frame respectively\n",
    "    # Note: Channels here would be refering to the no. of 3D solids outputted by the convolutions keeping, \n",
    "    #       the 'RGB' channels constant, essentially we are increasing the no. of frames per video as mentioned above\n",
    "    \n",
    "    # In this block the following are the layers:\n",
    "    # 1.'TimeDistributed 3x3 Conv2D',outputting 25 channels, activation 'relu'\n",
    "    # 2.'TimeDistributed 2x2 MaxPool2D'\n",
    "    # 3.'TimeDistributed Dropout layer', rate '0.2'\n",
    "    # This is repeated a second time on with the Conv2D layer outputting 35 channels\n",
    "    # and the Dropout layer has a rate of '0.3'\n",
    "    # This marks the end of the TimeDistributed 2D Convolutions\n",
    "    TimeDistributed(keras.layers.Conv2D(25, 3, activation= 'relu')),\n",
    "    TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), padding='same')),\n",
    "    TimeDistributed(keras.layers.Dropout(rate= 0.2)),\n",
    "    TimeDistributed(keras.layers.Conv2D(35, 3, activation= 'relu')),\n",
    "    TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), padding= 'same')),\n",
    "    TimeDistributed(keras.layers.Dropout(rate= 0.3)),\n",
    "    \n",
    "    # 3D Convolutions to convolve over the time axis\n",
    "    # Note: Channels here would be refering to the no. of 3D solids outputted by the convolutions,as mentioned above\n",
    "    \n",
    "    # Following are the layers:\n",
    "    # 1. '3x3x3 Conv3D', outputting 20 channels with an activation of 'relu'\n",
    "    # 2. '2x2x2 MaxPool3D',\n",
    "    # This is repeated again only changing the output channels for the 'Conv3D' to '20'\n",
    "    # This is followed by a Dropout layer with rate '0.2'\n",
    "    # This marks the end of the 3D convolutions\n",
    "    keras.layers.Conv3D(20, 3, activation= 'relu'),\n",
    "    keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding= 'same'),\n",
    "    keras.layers.Conv3D(20, 3, activation= 'relu'),\n",
    "    keras.layers.Dropout(rate= 0.2),\n",
    "    \n",
    "    # Convolution LSTM\n",
    "    # In this layer we use a kernel of '2' which means 2 in every direction or (2x2x2x2), we use the same regularizers in \n",
    "    # the above model along with same bias values\n",
    "    # we output 10 convolutions from this layer\n",
    "    keras.layers.ConvLSTM2D(10, \n",
    "                            2,\n",
    "                            kernel_initializer= 'normal',\n",
    "                            unit_forget_bias= True,\n",
    "                            activation= 'relu', \n",
    "                            kernel_regularizer= 'l2', \n",
    "                            recurrent_regularizer= 'l1', \n",
    "                            use_bias= True, \n",
    "                            bias_initializer= 'zeros',\n",
    "                           ),\n",
    "    \n",
    "    # TimeDistributed Flatten\n",
    "    TimeDistributed(keras.layers.Flatten()),\n",
    "    \n",
    "    # RNN Layers\n",
    "    # GRU Layer with TimeDistributed Output of 5\n",
    "    # activation= 'relu'; Reason: Proven to be good with image data also results above were favourable\n",
    "    # kernel_regularize= 'l2'; Reason: Experimentally prove optimal value\n",
    "    # recurrent_regularize= 'l2'; Reason: We changed it from 'l1' as above model had difficulty generalizing we also added\n",
    "    #                                    an extra dropout layer\n",
    "    # use_bias= True; Reason: Favorable results\n",
    "    # bias_initializer= zeros; Reason: Same as above \n",
    "    keras.layers.GRU(5, \n",
    "                     return_sequences= True, \n",
    "                     activation= 'relu', \n",
    "                     kernel_regularizer= 'l2',\n",
    "                     recurrent_regularizer= 'l2',\n",
    "                     use_bias= True,\n",
    "                     bias_initializer= 'zeros',\n",
    "                     ),\n",
    "    \n",
    "    # Flattening data\n",
    "    keras.layers.Flatten(),\n",
    "    \n",
    "    # Dense layer with softmax for output\n",
    "    keras.layers.Dense(5, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_28 (TimeDis (None, 25, 198, 198, 25)  700       \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 25, 99, 99, 25)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 25, 99, 99, 25)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 25, 97, 97, 35)    7910      \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 25, 49, 49, 35)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 25, 49, 49, 35)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 23, 47, 47, 20)    18920     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 12, 24, 24, 20)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 10, 22, 22, 20)    10820     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10, 22, 22, 20)    0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 21, 21, 10)        4840      \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 21, 210)           0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 21, 5)             3255      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 530       \n",
      "=================================================================\n",
      "Total params: 46,975\n",
      "Trainable params: 46,975\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model with:\n",
    "# optimizer: 'adam'\n",
    "# loss: 'categorical_crossentropy'\n",
    "# metrics: 'categorical_accuracy'\n",
    "# run_eagerly: True\n",
    "# Same as above\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics=['categorical_accuracy'], run_eagerly= True)\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced batch size\n",
    "batch_size= 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the train and val generators with:\n",
    "# batch_size: 15\n",
    "# im_size: 200\n",
    "# n_images: 20\n",
    "# we have set the default resize to 200 and n_images to 20\n",
    "# so even if we donot enter these values they will still be used\n",
    "\n",
    "train_generator = MyGen(train_path, batch_size, im_size=200, n_images= 25)\n",
    "val_generator = MyGen(val_path, batch_size, im_size=200, n_images= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model name\n",
    "model_name = 'model_2' + '_' + new_date_time + '.h5'\n",
    "    \n",
    "# Creating a path to store model\n",
    "if not os.path.exists(f'./models/{model_name}/'):\n",
    "    os.mkdir(f'./models/{model_name}/')\n",
    "    filepath = f'./models/{model_name}/'    \n",
    "else:\n",
    "    os.mkdir(f'./models/{model_name}_2/')\n",
    "    filepath = f'./models/{model_name}_2/'\n",
    "        \n",
    "# Setting up the model checkpoint and the callbacks_list\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=2, save_best_only=True, save_weights_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the steps per epoch for train and val generators\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int((num_train_sequences//batch_size) - 1)\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = int((num_val_sequences//batch_size) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.6010 - categorical_accuracy: 0.1876\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.21333, saving model to ./models/model_2_2021_01_04_23_10.h5_2\\\n",
      "43/43 [==============================] - 60s 1s/step - loss: 3.6010 - categorical_accuracy: 0.1876 - val_loss: 3.2013 - val_categorical_accuracy: 0.2133\n",
      "Epoch 2/45\n",
      "43/43 [==============================] - ETA: 0s - loss: 2.9087 - categorical_accuracy: 0.2481\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.21333 to 0.24000, saving model to ./models/model_2_2021_01_04_23_10.h5_2\\\n",
      "43/43 [==============================] - 57s 1s/step - loss: 2.9087 - categorical_accuracy: 0.2481 - val_loss: 2.6431 - val_categorical_accuracy: 0.2400\n",
      "Epoch 3/45\n",
      "43/43 [==============================] - ETA: 0s - loss: 2.4393 - categorical_accuracy: 0.2403\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.24000\n",
      "43/43 [==============================] - 48s 1s/step - loss: 2.4393 - categorical_accuracy: 0.2403 - val_loss: 2.2719 - val_categorical_accuracy: 0.2400\n",
      "Epoch 4/45\n",
      "43/43 [==============================] - ETA: 0s - loss: 2.0928 - categorical_accuracy: 0.2977\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.24000 to 0.30667, saving model to ./models/model_2_2021_01_04_23_10.h5_2\\\n",
      "43/43 [==============================] - 57s 1s/step - loss: 2.0928 - categorical_accuracy: 0.2977 - val_loss: 1.9721 - val_categorical_accuracy: 0.3067\n",
      "Epoch 5/45\n",
      "37/43 [========================>.....] - ETA: 5s - loss: 1.8728 - categorical_accuracy: 0.2973"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-b698c23bf38c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    804\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[1;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstep_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m       outputs = reduce_per_replica(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNext\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2607\u001b[1;33m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2608\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2609\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history= model.fit(\n",
    "    train_generator, \n",
    "    epochs= num_epochs,\n",
    "    steps_per_epoch= steps_per_epoch,\n",
    "    verbose= 1, \n",
    "    callbacks= callbacks_list, \n",
    "    validation_data= val_generator,\n",
    "    validation_steps= validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacc_2= history.history.get('val_categorical_accuracy')\n",
    "vloss_2= history.history.get('val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacc_2= history.history.get('categorical_accuracy')\n",
    "closs_2= history.history.get('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "# Latest Date time, this will be used for the checkpoints\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "time= str(curr_dt_time.time())\n",
    "date= str(curr_dt_time.date())\n",
    "T= time.split(':')\n",
    "new_time= '_' + T[0]+ '_' + T[1]\n",
    "new_date= date.replace('-', '_')\n",
    "new_date_time= new_date + new_time\n",
    "\n",
    "\n",
    "train_path = './Project_data/train/'\n",
    "val_path = './Project_data/val/'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.Sequential([\n",
    "    # Input layer for the model\n",
    "    # '20' is the no. of images/frames of the video selected for analysis\n",
    "    # '160' is the shape of the image, i.e. it is both the height and the width of the image\n",
    "    # '3' is the no. of channels RGB\n",
    "    \n",
    "    keras.layers.InputLayer(input_shape= (20,160,160,3)),\n",
    "    \n",
    "    # TimeDistributed 2D Convolutions to convolve each frame respectively\n",
    "    \n",
    "    # In this block the following are the layers:\n",
    "    # 1.'TimeDistributed 3x3 Conv2D',outputting 25 channels, activation 'relu'\n",
    "    # 2.'TimeDistributed 2x2 MaxPool2D'\n",
    "    # 3.'TimeDistributed Dropout layer', rate '0.2'\n",
    "    # This is repeated a second time on with the Conv2D layer outputting 35 channels\n",
    "    # and the Dropout layer has a rate of '0.3'\n",
    "    # This marks the end of the TimeDistributed 2D Convolutions\n",
    "    \n",
    "    TimeDistributed(keras.layers.Conv2D(25, 3, activation= 'relu')),\n",
    "    TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), padding='same')),\n",
    "    TimeDistributed(keras.layers.Dropout(rate= 0.2)),\n",
    "    \n",
    "    TimeDistributed(keras.layers.Conv2D(35, 3, activation= 'relu')),\n",
    "    TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), padding= 'same')),\n",
    "    TimeDistributed(keras.layers.Dropout(rate= 0.3)),\n",
    "    \n",
    "    # 3D Convolutions to convolve over the time axis\n",
    "    \n",
    "    # Following are the layers:\n",
    "    # 1. '3x3x3 Conv3D', outputting 25 channels with an activation of 'relu'\n",
    "    # 2. '2x2x2 MaxPool3D',\n",
    "    # This is repeated again only changing the output channels for the 'Conv3D' to 20 and reducing the kernel size to '3x3x3'\n",
    "    # This is followed by a Dropout layer with rate '0.2'\n",
    "    # This marks the end of the 3D convolutions\n",
    "    keras.layers.Conv3D(25, 3, activation= 'relu'),\n",
    "    keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding= 'same'),\n",
    "    \n",
    "    keras.layers.Conv3D(20, 2,activation= 'relu'),\n",
    "    keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding= 'same'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "   \n",
    "    # Flattening out the data in a TimeDistributed Layer\n",
    "    # This is done to preserve the data on the time axis\n",
    "    TimeDistributed(keras.layers.Flatten()),\n",
    "\n",
    "    # RNN Layers\n",
    "    \n",
    "    # A GRU layer with return_sequences= True\n",
    "    # This outputs each frame in a TimeDistributed manner\n",
    "    # kernel_regularizer= 'l2'; Reason: Found to be optimal\n",
    "    # recurrent_regularizer= 'l2'; Reason: Found to be optimal\n",
    "    # dropout= 0.3; Reason: Attempt to make model more robust\n",
    "    # recurrent_dropout= 0.2; Reason: Attempt to make model more robust\n",
    "    \n",
    "    keras.layers.GRU(30, \n",
    "                     return_sequences= True,\n",
    "                     activation= 'relu',\n",
    "                     kernel_regularizer= 'l2',\n",
    "                     recurrent_regularizer= 'l2',\n",
    "                     use_bias= True,\n",
    "                     bias_initializer= 'zeros',\n",
    "                     dropout= 0.3,\n",
    "                     recurrent_dropout= 0.2\n",
    "                     ),\n",
    "    \n",
    "    # A GRU layer with return_sequences= False\n",
    "    # This only produces outputs without the time axis, this can be imagined as viewing the whole video as a sequence\n",
    "    # This marks the end of the RNN layers\n",
    "    keras.layers.GRU(5, \n",
    "                     return_sequences= False, \n",
    "                     activation= 'softmax', \n",
    "                     recurrent_activation= 'sigmoid',\n",
    "                     kernel_regularizer= 'l2',\n",
    "                     recurrent_regularizer= 'l2',\n",
    "                     use_bias= True,\n",
    "                     bias_initializer= 'zeros',\n",
    "                     )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 20, 158, 158, 25)  700       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 79, 79, 25)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 79, 79, 25)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 77, 77, 35)    7910      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 20, 39, 39, 35)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 20, 39, 39, 35)    0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 18, 37, 37, 25)    23650     \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 9, 19, 19, 25)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 8, 18, 18, 20)     4020      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 4, 9, 9, 20)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 9, 9, 20)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 4, 1620)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 4, 30)             148680    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 5)                 555       \n",
      "=================================================================\n",
      "Total params: 185,515\n",
      "Trainable params: 185,515\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model with:\n",
    "# optimizer: 'adam'\n",
    "# loss: 'categorical_crossentropy'\n",
    "# metrics: 'categorical_accuracy'\n",
    "# run_eagerly: True\n",
    "# Same as above\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics=['categorical_accuracy'], run_eagerly= True)\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increased batch size\n",
    "batch_size= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the train and val generators with:\n",
    "# batch_size: 25\n",
    "# im_size: 160\n",
    "# n_images: 20\n",
    "\n",
    "train_generator = MyGen(train_path, batch_size, im_size=160, n_images= 20)\n",
    "val_generator = MyGen(val_path, batch_size, im_size=160, n_images= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model name\n",
    "model_name = 'model_3' + '_' + new_date_time + '.h5'\n",
    "    \n",
    "# Creating a path to store model\n",
    "if not os.path.exists(f'./models/{model_name}/'):\n",
    "    os.mkdir(f'./models/{model_name}/')\n",
    "    filepath = f'./models/{model_name}/'    \n",
    "else:\n",
    "    os.mkdir(f'./models/{model_name}_2/')\n",
    "    filepath = f'./models/{model_name}_2/'\n",
    "        \n",
    "# Setting up the model checkpoint and the callbacks_list\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=2, save_best_only=True, save_weights_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the steps per epoch for train and val generators\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int((num_train_sequences//batch_size) - 1)\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = int((num_val_sequences//batch_size) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 3.1753 - categorical_accuracy: 0.2320\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.33000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 41s 2s/step - loss: 3.1753 - categorical_accuracy: 0.2320 - val_loss: 2.6119 - val_categorical_accuracy: 0.3300\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 2.2848 - categorical_accuracy: 0.3152\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.33000\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.2848 - categorical_accuracy: 0.3152 - val_loss: 2.1015 - val_categorical_accuracy: 0.3200\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.8831 - categorical_accuracy: 0.4160\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.33000 to 0.43000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 41s 2s/step - loss: 1.8831 - categorical_accuracy: 0.4160 - val_loss: 1.8210 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.5865 - categorical_accuracy: 0.5424\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.43000\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.5865 - categorical_accuracy: 0.5424 - val_loss: 1.8870 - val_categorical_accuracy: 0.3600\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.3919 - categorical_accuracy: 0.5952\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.43000 to 0.65000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 41s 2s/step - loss: 1.3919 - categorical_accuracy: 0.5952 - val_loss: 1.3794 - val_categorical_accuracy: 0.6500\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.1222 - categorical_accuracy: 0.7184\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.65000\n",
      "25/25 [==============================] - 31s 1s/step - loss: 1.1222 - categorical_accuracy: 0.7184 - val_loss: 1.3246 - val_categorical_accuracy: 0.6400\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.9242 - categorical_accuracy: 0.7808\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.65000 to 0.68000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 41s 2s/step - loss: 0.9242 - categorical_accuracy: 0.7808 - val_loss: 1.2484 - val_categorical_accuracy: 0.6800\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.7796 - categorical_accuracy: 0.8320\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.68000 to 0.70000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.7796 - categorical_accuracy: 0.8320 - val_loss: 1.1404 - val_categorical_accuracy: 0.7000\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.7349 - categorical_accuracy: 0.8432\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.70000\n",
      "25/25 [==============================] - 31s 1s/step - loss: 0.7349 - categorical_accuracy: 0.8432 - val_loss: 1.1338 - val_categorical_accuracy: 0.6800\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6834 - categorical_accuracy: 0.8560\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.70000 to 0.72000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 41s 2s/step - loss: 0.6834 - categorical_accuracy: 0.8560 - val_loss: 1.0688 - val_categorical_accuracy: 0.7200\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6203 - categorical_accuracy: 0.8736\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.72000 to 0.75000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.6203 - categorical_accuracy: 0.8736 - val_loss: 1.0170 - val_categorical_accuracy: 0.7500\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.5444 - categorical_accuracy: 0.9040\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.75000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.5444 - categorical_accuracy: 0.9040 - val_loss: 1.0156 - val_categorical_accuracy: 0.7000\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.5139 - categorical_accuracy: 0.9216\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.75000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.5139 - categorical_accuracy: 0.9216 - val_loss: 1.0733 - val_categorical_accuracy: 0.7000\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4520 - categorical_accuracy: 0.9408\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.75000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.4520 - categorical_accuracy: 0.9408 - val_loss: 1.0569 - val_categorical_accuracy: 0.7100\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4879 - categorical_accuracy: 0.9168\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.75000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.4879 - categorical_accuracy: 0.9168 - val_loss: 0.9546 - val_categorical_accuracy: 0.7200\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3793 - categorical_accuracy: 0.9664\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.75000 to 0.77000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.3793 - categorical_accuracy: 0.9664 - val_loss: 0.9641 - val_categorical_accuracy: 0.7700\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4128 - categorical_accuracy: 0.9456\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.77000 to 0.78000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.4128 - categorical_accuracy: 0.9456 - val_loss: 0.7779 - val_categorical_accuracy: 0.7800\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3813 - categorical_accuracy: 0.9632\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.78000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3813 - categorical_accuracy: 0.9632 - val_loss: 0.9583 - val_categorical_accuracy: 0.7700\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3800 - categorical_accuracy: 0.9568\n",
      "Epoch 00019: val_categorical_accuracy improved from 0.78000 to 0.80000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.3800 - categorical_accuracy: 0.9568 - val_loss: 0.7770 - val_categorical_accuracy: 0.8000\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3619 - categorical_accuracy: 0.9664\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.80000 to 0.81000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.3619 - categorical_accuracy: 0.9664 - val_loss: 0.8650 - val_categorical_accuracy: 0.8100\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3379 - categorical_accuracy: 0.9712\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.81000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3379 - categorical_accuracy: 0.9712 - val_loss: 0.8707 - val_categorical_accuracy: 0.7700\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3460 - categorical_accuracy: 0.9728\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.81000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3460 - categorical_accuracy: 0.9728 - val_loss: 0.8229 - val_categorical_accuracy: 0.7900\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3311 - categorical_accuracy: 0.9712\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.81000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3311 - categorical_accuracy: 0.9712 - val_loss: 0.9134 - val_categorical_accuracy: 0.7800\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - ETA: 0s - loss: 0.3053 - categorical_accuracy: 0.9808\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.81000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3053 - categorical_accuracy: 0.9808 - val_loss: 0.9952 - val_categorical_accuracy: 0.7800\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3167 - categorical_accuracy: 0.9744\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.81000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3167 - categorical_accuracy: 0.9744 - val_loss: 0.9518 - val_categorical_accuracy: 0.7300\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3221 - categorical_accuracy: 0.9712\n",
      "Epoch 00026: val_categorical_accuracy improved from 0.81000 to 0.84000, saving model to ./models/model_3_2021_01_04_21_53.h5\\\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.3221 - categorical_accuracy: 0.9712 - val_loss: 0.7505 - val_categorical_accuracy: 0.8400\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2941 - categorical_accuracy: 0.9824\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.84000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.2941 - categorical_accuracy: 0.9824 - val_loss: 0.9584 - val_categorical_accuracy: 0.7500\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3568 - categorical_accuracy: 0.9632\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.84000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3568 - categorical_accuracy: 0.9632 - val_loss: 1.0598 - val_categorical_accuracy: 0.7900\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4240 - categorical_accuracy: 0.9440\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.84000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.4240 - categorical_accuracy: 0.9440 - val_loss: 0.8129 - val_categorical_accuracy: 0.7700\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3411 - categorical_accuracy: 0.9728\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.84000\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.3411 - categorical_accuracy: 0.9728 - val_loss: 0.9034 - val_categorical_accuracy: 0.7800\n"
     ]
    }
   ],
   "source": [
    "history= model.fit(\n",
    "    train_generator, \n",
    "    epochs= num_epochs,\n",
    "    steps_per_epoch= steps_per_epoch,\n",
    "    verbose= 1, \n",
    "    callbacks= callbacks_list, \n",
    "    validation_data= val_generator,\n",
    "    validation_steps= validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacc_3= history.history.get('val_categorical_accuracy')\n",
    "vloss_3= history.history.get('val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacc_3= history.history.get('categorical_accuracy')\n",
    "closs_3= history.history.get('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy and training accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (8, 4))\n",
    "plt.plot(vacc_1, label= 'val_categorical_acc')\n",
    "plt.plot(cacc_1, label= 'train_categorical_acc')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (8, 4))\n",
    "plt.plot(vacc_2, label= 'val_categorical_acc')\n",
    "plt.plot(cacc_2, label= 'train_categorical_acc')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (8, 4))\n",
    "plt.plot(vacc_3, label= 'val_categorical_acc')\n",
    "plt.plot(cacc_3, label= 'train_categorical_acc')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Loss and training Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (8, 4))\n",
    "plt.plot(vloss_1, label= 'val_categorical_acc')\n",
    "plt.plot(closs_1, label= 'train_categorical_acc')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (8, 4))\n",
    "plt.plot(vloss_2, label= 'val_categorical_acc')\n",
    "plt.plot(closs_2, label= 'train_categorical_acc')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (8, 4))\n",
    "plt.plot(vloss_3, label= 'val_categorical_acc')\n",
    "plt.plot(closs_3, label= 'train_categorical_acc')\n",
    "plt.legend(loc= 'lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
